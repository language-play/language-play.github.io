<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- <script src="https://distill.pub/template.v2.js"></script> -->
  <script src="lib/template.v2.js"></script>
  <!-- <script src="lib/blazy.js"></script> -->
  <!-- <script> -->
  <!--   var bLazy = new Blazy({ -->
  <!--       success: function(){ -->
  <!--           updateCounter(); -->
  <!--       } -->
  <!--   }); -->
  <!--   var imageLoaded = 0; -->
  <!--   function updateCounter() { -->
  <!--       imageLoaded++; -->
  <!--       console.log("blazy image loaded: "+imageLoaded); -->
  <!--   } -->
  <!-- </script> -->


  <style>
  </style>
</head>

<body>

  <d-front-matter>
    <script type="text/json">{
      "title": "Grounding Language in Play",
      "description": "Learning natural language conditioned control from play.",
      "authors": [
        {
          "author": "Corey Lynch*",
          "authorURL": "https://coreylynch.github.io",
          "affiliation": "Robotics at Google",
          "affiliationURL": "https://g.co/robotics"
        },
        {
          "author": "Pierre Sermanet*",
          "authorURL": "https://sermanet.github.io/home",
          "affiliation": "Robotics at Google",
          "affiliationURL": "https://g.co/robotics"
        },
        {
          "author": "* equal contribution",
            "authorURL": "",
            "affiliation": "",
          "affiliationURL": ""
        }
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  <style>
    figure {
      text-align: center;
      margin-bottom: 0.5em;
      margin-top: 0.5em;
    }
    figure img {
      max-width: 100%;
      width: unset;
    }
    video {
      max-width: 100%;
    }
    .colab-root {
      display: inline-block;
      background: rgba(255, 255, 255, 0.75);
      padding: 4px 8px;
      border-radius: 4px;
      font-size: 11px!important;
      text-decoration: none;
      color: #aaa;
      border: none;
      font-weight: 300;
      border: solid 1px rgba(0, 0, 0, 0.08);
      border-bottom-color: rgba(0, 0, 0, 0.15);
      text-transform: uppercase;
      line-height: 16px;
    }

   span.colab-span {
      background-image: url();
      background-repeat: no-repeat;
      background-size: 20px;
      background-position-y: 2px;
      display: inline-block;
      padding-left: 24px;
      border-radius: 4px;
      text-decoration: none;
    }

    a.colab-root:hover{
      color: #666;
      background: white;
      border-color: rgba(0, 0, 0, 0.2);
    }

    /* TOC */
    @media(max-width: 1000px){
      d-contents {
        justify-self: start;
        align-self: start;
        grid-column-start: 2;
        grid-column-end: 6;
        padding-bottom: 0.5em;
        margin-bottom: 1em;
        padding-left: 0.25em;
        border-bottom: 1px solid rgba(0, 0, 0, 0.1);
        border-bottom-width: 1px;
        border-bottom-style: solid;
        border-bottom-color: rgba(0, 0, 0, 0.1);
      }
    }

    @media (min-width: 1000px){
      d-contents {
        align-self: start;
        grid-column-start: 1;
        grid-column-end: 4;
        justify-self: end;
        padding-right: 3em;
        padding-left: 2em;
        border-right: 1px solid rgba(0, 0, 0, 0.1);
        border-right-width: 1px;
        border-right-style: solid;
        border-right-color: rgba(0, 0, 0, 0.1);
      }
    }

    @media (min-width: 1180px){
      d-contents {
        grid-column-start: 1;
        grid-column-end: 4;
        justify-self: end;
        padding-right: 3em;
        padding-left: 2em;
        border-right: 1px solid rgba(0, 0, 0, 0.1);
        border-right-width: 1px;
        border-right-style: solid;
        border-right-color: rgba(0, 0, 0, 0.1);
      }
    }

    d-contents nav h3 {
      margin-top: 0;
      margin-bottom: 1em;
    }

    d-contents nav a {
      color: rgba(0, 0, 0, 0.8);
      border-bottom: none;
      text-decoration: none;
    }

    d-contents li {
      list-style-type: none;
    }

    d-contents ul {
      padding-left: 1em;
    }

    d-contents nav ul li {
      margin-bottom: .25em;
    }

    d-contents nav a:hover {
      text-decoration: underline solid rgba(0, 0, 0, 0.6);
    }

    d-contents nav ul {
      margin-top: 0;
      margin-bottom: 6px;
    }


    d-contents nav>div {
      display: block;
      outline: none;
      margin-bottom: 0.5em;
    }

    d-contents nav>div>a {
      font-size: 13px;
      font-weight: 600;
    }

    d-contents nav>div>a:hover,
    d-contents nav>ul>li>a:hover {
        text-decoration: none;
    }

    /* code blocks to margins */
    @media (min-width: 1600px) {
      d-code {
        margin-top: -10px;
        grid-column-start: 12;
        grid-column-end: 14; 
      }
    }
    /* /\* so title is on one line *\/ */
    /* d-title h1, d-title p { */
    /*   grid-column: middle; */
    /* } */

  </style>
  <script>
  // hack to edit font size in code snippets. guaranteed a better way to do 
  // this, but I'm not a webdev
  window.onload = function() {
    setTimeout(() => { document.querySelectorAll("d-code").forEach(function(e) {e.shadowRoot.querySelector('#code-container').style.fontSize = "0.7em"}); }, 3000);
  }
  </script>

<!-- title section -->
<d-title>
  <h1>Grounding Language in Play</h1>
  <!-- <p>Combining open-ended robotic manipulation with open-ended human language conditioning</p> -->
  <p>Scalable human language conditioned visual control</p>

  <!--   <\!-- menu on the left -\-> -->
  <!-- <d-contents> -->
  <!-- <nav class="l-text toc figcaption"> -->
  <!--   <div><a href="https://arxiv.org/pdf/2005.TODO.pdf">pdf</a></div> -->
  <!--   <div><a href="#citation">bibtex</a></div> -->
  <!-- </nav> -->
  <!-- </d-contents> -->

  <!-- video -->
  <div class="l-body" id="demo">
  <figure>

    <video class="b-lazy" loop autoplay playsinline muted style="display: block; width: 80%; margin:0 auto"
           data-src="videos/live/playlang_20200326-193259_13tasks_bt300k.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
  <figcaption>
  <b>Video 1.</b> Our agent following real-time human language instructions.</b>
  </figcaption>
  </figure>
  </div>

</d-title>

<d-byline>
  <div class="byline grid">
    <div class="authors-affiliations grid">
      <h3>Authors</h3>
      <h3>Affiliations</h3>
        <p class="author"><a class="name" href="https://coreylynch.github.io">Corey Lynch*</a></p>
        <p class="affiliation"><a class="affiliation" href="https://g.co/robotics">Robotics at Google</a></p>
        <p class="author"><a class="name" href="https://sermanet.github.io/home">Pierre Sermanet*</a></p>
        <p class="affiliation"><a class="affiliation" href="https://g.co/robotics">Robotics at Google</a></p>
        <p>* equal contribution</p>
    </div>
    <div>
      <h3>Published</h3>
        <p>May 18, 2020</p> 
    </div>
    <div>
      <h3>Links</h3>
        <p><a href="https://arxiv.org/pdf/2005.07648.pdf">pdf</a></p>
        <p><a href="#citation">bibtex</a></p>
    </div>
  </div>
</d-byline>

<d-article>

<!-- contents links -->
<d-contents>
  <nav class="l-text toc figcaption">
    <h3>Contents</h3>
    <div><a href="#introduction">1. Introduction</a></div>
      <ul><li><a href="#demo"><u>Video 1:</u> Following human language instructions</a></li></ul>
      <ul><li><a href="#multilingual_teaser"><u>Video 2:</u> Zero shot instructions, 16 languages</a></li></ul>
    <div><a href="#related">2. Related Work</a></div>
    <div><a href="#preliminaries">3. Preliminaries</a></div>
    <!-- <ul> -->
    <!--   <li><a href="#prelim1">3.1 Relabeled imitation learning</a></li> -->
    <!--   <li><a href="#prelim2">3.2 Teleoperated play</a></li> -->
    <!--   <li><a href="#prelim3">3.3 Learning from play</a></li> -->
    <!-- </ul> -->
    <div><a href="#sec:4">4. Learning to Follow Human Language Instructions</a></div>
    <ul>
      <li><a href="#sec:hip">4.1 Pairing robot experience with language</a></li>
      <ul><li><a href="#video:data"><u>Video 3:</u> Training Data</a></li></ul>
      <li><a href="#sec:mcil">4.2 Multicontext Imitation Learning</a></li>
      <li><a href="#sec:lang_lfp">4.3 LangLfP: following image and language goals.</a></li>
      <li><a href="#sec:transfer_lang_lfp">4.4 Transferring knowledge from text corpora.</a></li>
    </ul>
    <div><a href="#experimental_setup">5. Experimental Setup</a></div>
    <!-- <ul> -->
    <!--   <li><a href="#sec:sim_env">5.1 3D simulated environment</a></li> -->
    <!--   <li><a href="#sec:methods">5.2 Methods</a></li> -->
    <!-- </ul> -->
    <div><a href="#ama">6. "Ask Me Anything" Experiments</a></div>
    <ul>
      <li><a href="#sec:long_horizon_eval">6.1 Long-Horizon Evaluation</a></li>
      <li><a href="#sec:long_horizon_results">6.2 Long Horizon Results</a></li>
      <ul>
        <li><a href="#qualitative-langlfp"><u>Video 4:</u> Test time human language conditioned visual control</a></li>
        <li><a href="#15tasks"><u>Video 5:</u> 15 tasks in a row</a></li>
        <li><a href="#lfp-vs-bc"><u>Video 6:</u> Play vs. conventional demonstrations</a></li>
        <li><a href="#human_assistance"><u>Video 7:</u> Language unlocks human assistance</a></li>
        <li><a href="#ood_tasks"><u>Video 8:</u> Composing new tasks with language</a></li>
      </ul>
    </ul>
    <div><a href="#transfer">7. Knowledge Transfer Experiments</a></div>
    <ul>
      <li><a href="#multilingual"><u>Video 9:</u> Transfer unlocks zero shot instruction following</a></li>
    </ul>
    <div><a href="#future">8. Limitations and Future Work</a></div>
    <div><a href="#conclusion">9. Conclusion</a></div>
    <div><a href="#appendix">Appendix</a></div>
    <div><a href="#acknowledgments">Acknowledgments & References</a></p>
<!--     <ul>
      <li><a href="#relabeling">A. Relabeling play</a></li>
      <li><a href="#sec:appendix_architecture">B. LangLfP Implementation Details</a></li>
      <li><a href="#sec:appendix_env">C. Environment</a></li>
      <li><a href="#sec:appendix_dataset">D. Datasets</a></li>
      <li><a href="#sec:appendix_models">E. Models</a></li>
      <li><a href="#sec:appendix_eval">F. Long Horizon Evaluation</a></li>
      <li><a href="#sec:qualitative_examples">G. Qualitative Examples</a></li>
      <ul>
        <li><a href="#ood_tasks"><u>Videos:</u>Tasks not present in the 18-task benchmark</a></li>
      </ul>
      <li><a href="#ablation">H. Ablation: How much language is necessary?</a></li>
    </ul> -->
<!--     <div><a href="#acknowledgments">Acknowledgments</a></div>
    <div><a href="#references">References</a></div>
 -->    
    <!-- <ul> -->

      
    <!--   <li><a href="#long-horizon">15 tasks in a row</a></li> -->
    <!--   <li><a href="#lfp-vs-bc">Learning from play vs demonstrations</a></li> -->
    <!--   <li><a href="#multilingual">Multi-lingual inputs</a></li> -->
    <!--   <li><a href="#transfer">Knowledge transfer via language</a></li> -->
    <!--   <li><a href="#repeated">Repeated instructions</a></li> -->
    <!--   <li><a href="#ood_tasks">Tasks not in benchmark</a></li> -->
    <!--   <li><a href="#interactive">Operator can help robot</a></li> -->
    <!--   <li><a href="#failures">Failure cases</a></li> -->
    <!--   <li><a href="#more_examples">Short sequences</a></li> -->
    <!-- </ul> -->
  </nav>
</d-contents>

<!-- starting article -->
<p>
  <b>Abstract</b><br>
Natural language is perhaps the most versatile and intuitive way for humans to communicate tasks to a robot.
Prior work on <a href="https://learning-from-play.github.io/">Learning from Play</a> (LfP) <d-cite key="lynch2019play"></d-cite> provides a simple approach for learning a wide variety of robotic behaviors from general sensors. However, each task must be specified with a goal image&mdash;something that is not practical in open-world environments.
In this work we present a simple and scalable way to condition policies on human language instead.
We extend LfP by pairing short robot experiences from play with relevant human language after-the-fact.
To make this efficient, we introduce <i>multicontext imitation</i>, which allows us to train a single agent to follow image or language goals, then use just language conditioning at test time.
This reduces the cost of language pairing to less than 1% of collected robot experience, with the majority of control still learned via self-supervised imitation.
At test time, a single agent trained in this manner can perform many different robotic manipulation skills in a row in a 3D environment, directly from images, and specified only with natural language (e.g. "open the drawer...now pick up the block...now press the green button...") (<a href="#long-horizon">video</a>). Finally, we introduce a simple technique that transfers knowledge from large unlabeled text corpora to robotic learning.
We find that transfer significantly improves downstream robotic manipulation. It also allows our agent to follow thousands of novel instructions at test time in zero shot, in 16 different languages. See videos of our experiments below.
</p>

<div class="l-page-outset" id="teaser">
<figure>
<img id="fig:teaser" src="figs/teaser/teaser_mockup8.png" style="margin: 0; width: 200%;"/>
  <figcaption>
  <b>Figure 1. Learning to follow natural language instructions from play:</b> 1) First, relabel teleoperated play into many image goal examples. Next, pair a small amount of play with hindsight instructions, yielding language goal examples. 2) Multicontext imitation: train a single policy on both image and language goals. 3) Test time: A single agent performs many skills in a row, directly from images, specified only in natural language.
  </figcaption>
</figure>
</div>

<h3 id="introduction">1. Introduction</h3>
<p>
A long-term motivation in robotic learning is the idea of a generalist robot&mdash;a single agent that can solve many tasks in everyday settings, using only general onboard sensors. A fundamental but less considered aspect, alongside task and observation space generality, is <i>general task specification</i>: the ability for untrained users to direct agent behavior using the most intuitive and flexible mechanisms. Along these lines, it is hard to imagine a truly generalist robot without also imagining a robot that can follow instructions expressed in natural language.
</p>

<p>
More broadly, children learn language in the context of a rich, relevant, sensorimotor experience <d-cite key="sachs1981language,smith2005development"></d-cite>. This motivates the long-standing question in artificial intelligence of <i>embodied language acquisition</i> <d-cite key="harnad1990symbol,mooney2008learning"></d-cite>: how might intelligent agents ground language understanding in their own embodied perception? An ability to relate language to the physical world potentially allows robots and humans to communicate on common ground over shared sensory experience<d-cite key="clark1991grounding, 2004.10151"></d-cite>&mdash;something that could lead to much more meaningful forms of human-machine interaction.
</p>

<p>
Furthermore, language acquisition, at least in humans, is a highly <i>social</i> process<d-cite key="skinner1957verbal,baldwin2007inherently,verga2013relevant"></d-cite>. During their earliest interactions, infants contribute actions while caregivers contribute relevant words<d-cite key="clark1991grounding"></d-cite>. While we cannot speculate on the actual learning mechanism at play in humans, we are interested in what robots can learn from similar paired data. In this work, we ask: assuming real humans play a critical role in robot language acquisition, what is the most efficient way to go about it? How can we scalably pair robot experience with relevant human language to bootstrap instruction following?
</p>

<div class="l-body" id="mcil">
<figure>
<img src="figs/mcil/mcil.png" style="margin: 0; width: 80%;"/>
  <figcaption>
  <b>Figure 2. Multicontext Imitation Learning:</b> 
  We introduce Multi-context Imitation Learning (MCIL), a simple generalization of contextual imitation learning to multiple heterogeneous contexts. An increasingly typical scenario in imitation learning is one where we have <i>multiple</i> imitation datasets, each with a different task description (e.g. goal image, task id, natural language, video demonstration, etc.) and different cost of collection. MCIL trains: 1) a single <i>latent goal</i> conditioned policy over all datasets, and 2) a set of encoders, one per dataset, each responsible for mapping a particular task description to the shared latent space.
  </figcaption>
</figure>
</div>

<p>
Even simple instruction following, however, poses a notoriously difficult learning challenge, subsuming many long-term problems in AI <d-cite key="hermann2017grounded,das2018embodied"></d-cite>.
For example, a robot presented with the command "sweep the block into the drawer” must be able to relate language to low-level perception (what does a block look like? what is a drawer?). It must perform visual reasoning (what does it mean for the block to be in the drawer?). Finally, it must solve a complex sequential decision problem (what commands do I send to my arm to "sweep"?). We note these questions cover only a single task, whereas the generalist robot setting demands single agents that perform many.
</p>

<p>
In this work, we combine the setting of open-ended robotic manipulation with open-ended human language conditioning. There is a long history of work studying instruction following (survey <d-cite key="luketina2019survey"></d-cite>). Prior work typically studies restricted observation spaces (e.g. games <d-cite key="1903.02020"></d-cite>, 2D gridworlds <d-cite key="yu2018interactive"></d-cite>), simplified actuators, (e.g. binary pick and place primitives <d-cite key="hill2019emergent"></d-cite>), and synthetic language data <d-cite key="hermann2017grounded"></d-cite>. We study the first combination, to our knowledge, of 1) human language instructions, 2) high-dimensional continuous sensory inputs and actuators, and 3) complex tasks like long-horizon robotic object manipulation. The test time scenario we consider is a single agent that can perform many tasks in a row, each specified by a human in natural language. For example, "open the door all the way to the right...now pick up the block...now push the red button...now close the door". Furthermore, the agent should be able to perform any combination of subtasks in any order. We refer to this as the "ask me anything" scenario (<a href="#fig:teaser">Figure 1</a>, step 3), which tests three important aspects of generality: general-purpose control, learning from onboard sensors, and general task specification.
</p>

<p>
Prior work on <a href="https://learning-from-play.github.io/">Learning from Play</a> (LfP) <d-cite key="lynch2019play"></d-cite> provides a simple starting point for learning general-purpose skills from onboard sensors: first cover state space with teleoperated "play" data, then use relabeled imitation learning to distill many reusable behaviors into a goal-directed policy. However, like other methods that combine relabeling with image observations <d-cite key="warde2018unsupervised, nair2018visual, pong2019skew, nair2019contextual, florensa2019self"></d-cite>, LfP requires that tasks be specified using a goal image to reach. While trivial in a simulator, this form of task specification is often impractical in open-world environments.
</p>

<p>
In this work, we present a simple approach that extends LfP to the natural language conditioned setting. <a href="#fig:teaser">Figure 1</a> provides an overview, which has four main steps:
<ol>
  <li>
    <b>Cover the space with teleoperated play</b> (Figure 1, step 1): Collect a teleoperated <a href="https://learning-from-play.github.io/assets/mp4/play_data516x360_bt300k.mp4">"play"</a> dataset. These long temporal state-action logs are automatically relabeled into many short-horizon demonstrations, solving for image goals.
  </li>
  <li>
    <b>Pair play with human language</b> (Figure 1, step 1): Typical setups pair instructions with optimal behavior. Instead, we pair any behavior from play after-the-fact with optimal <i>instructions</i>, a process we call Hindsight Instruction Pairing. This collection method is the first contribution of this paper. It yields a dataset of demonstrations, solving for human language goals.
  </li>
  <li>
    <b>Multicontext imitation learning</b> (Figure 1, step 2): We train a single policy to solve image or language goals, then use only language conditioning at test time. To make this possible, we introduce Multicontext Imitation Learning (Figure 2). This is the second contribution of this paper. This approach is highly data efficient. It reduces the cost of language pairing to less than 1% of collected robot experience to enable language conditioning, with the majority of control still learned via self-supervised imitation.
  </li>
  <li>
    <b>Condition on human language at test time</b> (Figure 1, step 3): At test time, our experiments show that a single policy trained in this manner can perform many complex robotic manipulation skills in a row, directly from images, and specified entirely with natural language (see <a href="#long-horizon">video</a>)
  </li>
</ol>
</p>

<p>
Finally, we study <b>transfer learning from unlabeled text corpora to robotic manipulation</b>. We introduce a simple transfer learning augmentation, applicable to any language conditioned policy. We find that this significantly improves downstream robotic manipulation. This is the first instance, to our knowledge, of this kind of transfer. Importantly, this same technique allows our agent to follow <a href="#multilingual_teaser">thousands of novel instructions in zero shot</a>, across 16 different languages. This is the third contribution of this work.
</p>


      <figure id="multilingual_teaser">
        <video id="video:2" class="b-lazy" loop autoplay playsinline muted style="display: block; width: 80%; margin:0 auto" data-src="videos/languages/multilingual_bt300k.mp4" type="video/mp4"></video>
        <figcaption><b>Video 2: Following out-of-distribution instructions in zero shot, in 16 languages.</b></figcaption>
      </figure>


<h3 id="related">2. Related Work</h3>
<p>
<b>Robotic learning from general sensors</b>.
In general, learning complex robotic skills from low-level sensors is possible, but requires substantial human supervision. Two of the most successful approaches are imitation learning (IL) <d-cite key="atkeson1997robot, argall2009survey"></d-cite> and reinforcement learning (RL) <d-cite key="kober2013reinforcement,levine2016end,kalashnikov2018qt,haarnoja2018soft,andrychowicz2020learning"></d-cite>. IL typically requires many human demonstrations <d-cite key="duan2017one,rajeswaran2017learning,rahmatizadeh2018vision,zhang2018deep"></d-cite> to drive supervised learning of a policy. In RL, supervision takes the form of a well-shaped <d-cite key="popov2017data"></d-cite> hand-designed reward to drive autonomous trial and error learning. Designing reward functions in open-world environments is non-trivial, requiring either task-specific instrumentation <d-cite key="gu2017deep"></d-cite> or learned perceptual rewards <d-cite key="Sermanet2017Rewards,Sermanet2017TCN,singh2019end"></d-cite>. Additionally, RL agents face difficult exploration problems <d-cite key="thrun1992efficient"></d-cite>, often requiring hand-designed exploration strategies <d-cite key="kalashnikov2018qt,ebert2018visual"></d-cite> or even human demonstrations as well <d-cite key="rajeswaran2017learning,hester2018deep"></d-cite>. Finally, conventional IL and RL only account for the training of a single task. Even under multitask formulations of RL <d-cite key="teh2017distral"></d-cite> and IL <d-cite key="rahmatizadeh2018vision"></d-cite>, each new task considered requires a corresponding and sizable human effort. RL additionally faces significant optimization challenges <d-cite key="schaul2019ray"></d-cite> in the multitask setting, often leading to worse data efficiency than learning tasks individually <d-cite key="parisotto2015actor"></d-cite>. This makes it difficult to scale either approach naively to the broad task setting of a generalist robot.
</p>

<p>
<b>Task-agnostic control</b>. This paper builds on the setting of task-agnostic control, where a single agent must be able to reach any reachable goal state in its environment upon command <d-cite key="kaelbling1993learning, schaul2015universal,warde2018unsupervised"></d-cite>. One way of acquiring this kind of control is to first learn a model of the environment through interaction <d-cite key="oh2015action,hafner2018learning,ebert2018visual,ha2018world"></d-cite> then use it for planning. However, these approaches face the same intractable autonomous exploration problem that RL does, requiring either environments simple enough to explore fully with random actions or hand-scripted primitives to guide richer behavior. A powerful model-free strategy for task-agnostic control is goal relabeling <d-cite key="kaelbling1993learning,andrychowicz2017hindsight"></d-cite>. This self-supervised technique trains goal conditioned policies to reach any previously visited state upon demand, with many recent examples in RL <d-cite key="nair2018visual,pong2018temporal,gupta2019relay,pong2019skew,ghosh2019learning,eysenbach2020rewriting} and IL \cite{lynch2019play,ding2019goal"></d-cite>. However, when combined high dimensional observation spaces like images, relabeling results in policies that must be instructed with goal images at test time&mdash;a prohibitively expensive assumption in open world (non-simulator) environments. The present work builds heavily on relabeled imitation, but additionally equips policies with flexible natural language conditioning.
</p>

<p>
<b>Covering state space</b>.
Learning generally requires exposure to diverse training data. While relabeling allows for goal-directed learning from any experience, it cannot account for the <i>diversity</i> of that experience, which comes entirely from the underlying data. In task agnostic control, where agents must be prepared to reach any user-proposed goal state at test time, an effective data collection strategy is one that visits as many states as possible during training <d-cite key="pong2019skew"></d-cite>. Obtaining this sort of state space coverage autonomously is a long-standing problem <d-cite key="thrun1992efficient"></d-cite> motivating work on tractable heuristics <d-cite key="kolter2009near"></d-cite>, intrinsic motivation <d-cite key="oudeyer2008can,journals/tamd/Schmidhuber10"></d-cite>, and more recent information-theoretic approaches <d-cite key="pong2019skew,lee2019efficient"></d-cite>
. Success so far has been limited to restricted environments <d-cite key="pathak2017curiosity"></d-cite>, as automatic exploration quickly becomes intractable in complex settings like object manipulation, where even knowing which states are valid presents a major challenge <d-cite key="pong2019skew"></d-cite>. Teleoperated play data <d-cite key="lynch2019play"></d-cite> sidesteps this exploration problem entirely, using prior human knowledge of object affordances to cover state space. This type of data stands in contrast to conventional teleoperated multitask demonstrations <d-cite key="zhang2018deep,handa2019dexpilot,singh2020scalable"></d-cite>, whose diversity is necessarily constrained by an upfront task definition. Play data has served as the basis for relabeled IL <d-cite key="lynch2019play"></d-cite> and relabeled hierarchical RL <d-cite key="gupta2019relay"></d-cite>. Play combined with relabeled IL is a central component of this work.
</p>

<p>
<b>Multicontext learning</b>. A number of previous methods have focused on generalizing across tasks <d-cite key="caruana1997multitask"></d-cite>, or generalizing across goals <d-cite key="schaul2015universal"></d-cite>. We introduce multicontext learning, a framework for generalizing across heterogeneous task and goal <i>descriptions</i>, e.g. goal image and natural language. When one of the training sources is plentiful and the other scarce, multicontext learning can be seen as transfer learning <d-cite key="tan2018survey"></d-cite> through a shared goal space. Multicontext imitation is a central component of our method, as it reduces the cost of human language supervision to the point where it can be practically applied to learn open-ended instruction following.
</p>

<p>
<b>Instruction following</b>. There is a long history of research into agents that not only learn a grounded language understanding <d-cite key="winograd1972understanding,mooney2008learning"></d-cite>, but demonstrate that understanding by following instructions <d-cite key="luketina2019survey"></d-cite>. Recently, authors have had success using deep learning to directly map raw input and text instructions to actions. However, prior work often studies restricted environments <d-cite key="macmahon2006walk,kollar2010toward,1611.01796,oh2017zero,misra2017mapping,yu2017deep,yu2018interactive"></d-cite> and simplified actuators <d-cite key="wang2019reinforced,hill2019emergent,chaplot2018gated,1903.02020,ALFRED20"></d-cite>. Additionally, learning to follow <i>natural</i> language is still not the standard in instruction-following research <d-cite key="luketina2019survey"></d-cite>, with typical implementations instead assuming access to simulator-provided instructions drawn from a restricted vocabulary and grammar <d-cite key="hermann2017grounded,1906.07343"></d-cite>.
This work, in contrast, studies 1) natural language instructions, 2) high-dimensional continuous sensory inputs and actuators, and 3) complex tasks like long-horizon 3D robotic object manipulation. Furthermore, unlike existing RL approaches to instruction following, our IL method is highly sample efficient, requires no reward definition, and trivially scales to the multitask setting.
</p>

<p>
<b>Transfer learning from generic text corpora</b>. Transfer learning from large "in the wild" text corpora, e.g. Wikipedia, to downstream tasks is prevalent in NLP <d-cite key="devlin2018bert,1801.06146,radford2019language, raffel2019exploring,1901.05287,1905.06316"></d-cite>, but has been relatively unexplored for language-guided control. An identified reason for this gap <d-cite key="luketina2019survey"></d-cite> is that tasks studied so far use small synthetic language corpora and are typically too artificial to benefit from transfer from real-world textual corpora. Like <d-cite key="ALFRED20"></d-cite>, we study instruction following in a simulated 3D home with real-world semantics, but expand the setting further to robotic object manipulation under realistic physics. This allows us to show the first example, to our knowledge, of positive transfer from "in the wild" text corpora to instruction following, making our agent better at language-guided manipulation and allowing it to follow many instructions outside its training set.
</p>

<h3 id="preliminaries">3. Preliminaries</h3>
We first review the prior settings of relabeled imitation learning and LfP, then introduce LangLfP, our natural language extension of LfP which builds on these components.

<h4 id="prelim1">3.1 Relabeled imitation learning</h4>
<p>
Goal conditioned learning <d-cite key="kaelbling1993learning"></d-cite> trains a single agent to reach any goal. This is formalized as a goal conditioned policy $\pi_{\theta}(a | s, g)$, which outputs next action $a \in A$, conditioned on current state $s \in S$ and a task descriptor $g \in G$. Imitation approaches learn this mapping using supervised learning over a dataset $\mathcal{D} = \{(\tau,g)_i\}^N_i$, of expert state-action trajectories $\tau = \{(s_0, a_0), ... \}$ solving for a paired task descriptor (typically a one-hot task encoding <d-cite key="rahmatizadeh2018vision"></d-cite>).
A convenient choice for a task descriptor is some goal state $g = s_g \in S$ to reach. This allows any state visited during collection to be <i>relabeled</i> <d-cite key="kaelbling1993learning,andrychowicz2017hindsight"></d-cite> as a "reached goal state", with the preceding states and actions treated as optimal behavior for reaching that goal. Applied to some original dataset $\mathcal{D}$, this yields a much larger dataset of relabeled examples $\mathcal{D}_R = \{(\tau, s_g)_i\}^{N_{R}}_i$, $N_{R} >> N$, providing the inputs to a simple maximum likelihood objective for goal directed control: relabeled goal conditioned behavioral cloning (GCBC) <d-cite key="lynch2019play, ding2019goal"></d-cite>:
</p>

<div class="l-body" id="gcbc">
<figure>
<img id="eq:gcbc" src="eqs/gcbc.png" style="margin: 0; width: 70%;"/>
</figure>
</div>

<p>
While relabeling automatically generates a large number of goal-directed demonstrations at training time, it cannot account for the <i>diversity</i> of those demonstrations, which comes entirely from the underlying data. To be able to reach <i>any</i> user-provided goal, this motivates data collection methods, upstream of relabeling, that fully cover state space.
</p>

<h4 id="prelim2">3.2 Teleoperated play</h4>
<p>
Human teleoperated "play" collection <d-cite key="lynch2019play"></d-cite>  directly addresses the state space coverage problem. In this setting, an operator is no longer constrained to a set of predefined tasks, but rather engages in every available object manipulation in a scene <a href="https://learning-from-play.github.io/assets/mp4/play_data516x360_bt300k.mp4">(example of play data)</a>. The motivation is to fully cover state space using prior human knowledge of object affordances. During collection, the stream of onboard robot observations and actions are recorded, $\{(s_t, a_t)\}^{\infty}_{t=0}$, yielding an unsegmented dataset of unstructured but semantically meaningful behaviors, useful in a relabeled imitation learning context.
</p>

<h4 id="prelim3">3.3 Learning from play</h4>
<p>
LfP <d-cite key="lynch2019play"></d-cite> combines relabeled imitation learning with teleoperated play. First, unsegmented play logs are relabeled using <a href="#alg2">Algorithm 2</a>. This yields a training set $D_{\mathrm{play}} = \{(\tau, s_g)_i\}^{D_{\mathrm{play}}}_{i=0}$, holding many diverse, short-horizon examples. These can be fed to a standard maximum likelihood goal conditioned imitation objective:
</p>

<div class="l-body" id="eq-lfp">
<figure>
<img id="eq:lfp" src="eqs/lfp.png" style="margin: 0; width: 70%;"/>
</figure>
</div>

<h3 id="sec:4">4. Learning to Follow Human Language Instructions</h3>
<p>
A limitation of LfP&mdash;and other approaches that combine relabeling with image state spaces&mdash;is that behavior must be conditioned on a goal image $s_g$ at test time. In this work, we focus on a more flexible mode of conditioning: humans describing tasks in natural language. Succeeding at this requires solving a complicated grounding problem. To address this, we introduce Hindsight Instruction Pairing (<a href="#sec:hip">Section 4.1</a>), a method for pairing large amounts of diverse robot sensor data with relevant human language. To leverage both image goal and language goal datasets, we introduce multicontext imitation learning (<a href="#sec:mcil">Section 4.2</a>). In (<a href="#sec:lang_lfp">Section 4.3</a>), we describe LangLfP which ties together these components to learn a single policy that follows many human instructions over a long horizon.
</p>

<h4 id="sec:hip">4.1 Pairing robot experience with human language</h4>
<p>
From a statistical machine learning perspective, an obvious candidate for grounding human language in robot sensor data is a large corpora of robot sensor data paired with relevant language. One way to collect this data is to choose an instruction, then collect optimal behavior. Instead we sample any robot behavior from play, then collect an optimal instruction. We call this Hindsight Instruction Pairing (<a href="#alg3">Algorithm 3</a>, part 1 of <a href="#teaser">Figure 1</a>). Just like how a hindsight goal image is an after-the-fact answer to the question "which goal state makes this trajectory optimal?", a <i>hindsight instruction</i> is an after-the-fact answer to the question "which language instruction makes this trajectory optimal?". We obtain these pairs by showing humans onboard robot sensor videos, then asking them "what instruction would you give the agent to get from first frame to last frame?" See examples in <a href="#video:data">Video 3</a>.
</p>


<figure>
<video id="video:data" class="b-lazy" loop autoplay playsinline muted style="display: block; width: 70%; margin:0 auto" data-src="videos/data/playground_all_32_sequences_6_bt300k.mp4" type="video/mp4"></video>
  <figcaption>
  <b>Video 3. Examples of our training data:</b> We sample random experiences from teleoperated play, then ask humans: "what instruction would you give the agent to get from first frame to last frame?" 
  </figcaption>
</figure>


<p>
Concretely, our pairing process assumes access to $D_{\mathrm{play}}$, obtained using 
<a href="#alg2">Algorithm 2</a> and a pool of non-expert human overseers. From $D_{\mathrm{play}}$, we create a new dataset $D_{\mathrm{(play,lang)}} = \{(\tau, l)_i\}^{D_{\mathrm{(play,lang)}}}_{i=0}$, consisting of short-horizon play sequences $\tau$ paired with $l \in L$, a human-provided hindsight instruction with no restrictions on vocabulary or grammar.
</p>

<p>
This process is scalable because pairing happens after-the-fact, making it straightforward to parallelize via crowdsourcing. The language collected is also naturally rich, as it sits on top of play and is similarly not constrained by an upfront task definition. This results in instructions for functional behavior (e.g. "open the drawer", "press the green button"), as well as general non task-specific behavior (e.g. "move your hand slightly to the left." or "do nothing.") See more real examples in <a href="#tab:ex_train_lang">Table 5</a> and video examples of our paired training data <a href="#sec:training_data">here</a>. Crucially, we do not need pair every experience from play with language to learn to follow instructions. This is made possible with multicontext imitation learning, described next.
</p>

<div class="l-body" id="alg1">
<figure>
<img id="alg1" src="algs/alg1.png" style="margin: 0; width: 70%;"/>
  <figcaption>
  <b>Algorithm 1: Multicontext Imitation Learning.</b>
  </figcaption>
</figure>
</div>

<h4 id="sec:mcil">4.2 Multicontext imitation learning</h4>
<p>
So far, we have described a way to create two contextual imitation datasets: $D_{\mathrm{play}}$ holding hindsight goal image examples and $D_{\mathrm{(play,lang)}}$, holding hindsight instruction examples. Ideally, we could train a single policy that is agnostic to either task description. This would allow us to share statistical strength over multiple datasets during training, then free us to use just language specification at test time.
</p>

<p>
With this motivation, we introduce <i>multicontext imitation learning</i> (MCIL), a simple and universally applicable generalization of contextual imitation to multiple heterogeneous contexts. The main idea is to represent a large set of policies by a single, unified function approximator that generalises over states, tasks, and <i>task descriptions</i>. Concretely, MCIL assumes access to multiple imitation learning datasets $\mathcal{D} = \{D^0, ...,\ D^K\}$, each with a different way of describing tasks. Each $D^k = \{(\tau^{k}_i, c_{i}^k)\}^{D^k}_{i=0}$ holds pairs of state-action trajectories $\tau$ paired with some context $c \in C$. For example, $D^0$ might contain demonstrations paired with one-hot task ids (a conventional multitask imitation learning dataset), $D^1$ might contain image goal demonstrations, and $D^2$ might contain language goal demonstrations.
</p>

<p>
Rather than train one policy per dataset, MCIL instead trains a single <i>latent goal</i> conditioned policy $\pi_{\theta}(a_t | s_t, z)$ over <i>all</i> datasets simultaneously, learning to map each task description type to the same latent goal space $z \in \mathbb{R}^d$. See <a href="#mcil">Figure 2</a>. This latent space can be seen as a common abstract goal representation shared across many imitation learning problems. To make this possible, MCIL assumes a set of parameterized encoders $\mathcal{F} = \{f_{\theta}^0, ..., f_{\theta}^K\}$, one per dataset, each responsible for mapping task descriptions of a particular type to the common latent goal space, i.e. $z = f_{\theta}^k(c^k)$. For instance these could be a task id embedding lookup, an image encoder, and a language encoder respectively.
</p>

<p>
MCIL has a simple training procedure: At each training step, for each dataset $D^k$ in $\mathcal{D}$, sample a minibatch of trajectory-context pairs $(\tau^k, c^k) \thicksim D^k$, encode the contexts in latent goal space $z = f_{\theta}^k(c^k)$, then compute a simple maximum likelihood contextual imitation objective:
</p>
<div class="l-body" id="mcil1">
<figure>
<img id="mcil1" src="eqs/mcil1.png" style="margin: 0; width: 70%;"/>
</figure>
</div>
<p>
The full MCIL objective averages this per-dataset objective over all datasets at each training step,
</p>
<div class="l-body" id="mcil2">
<figure>
<img id="mcil2" src="eqs/mcil2.png" style="margin: 0; width: 70%;"/>
</figure>
</div>
<p>
and the policy and all goal encoders are trained end to end to maximize $\mathcal{L}_{\textrm{MCIL}}$.  See <a href="#alg1">Algorithm 1</a> for full minibatch training pseudocode.
<p/>
<p>
Multicontext learning has properties that make it broadly useful beyond this paper. While we set $\mathcal{D}  = \{D_{\mathrm{play}}, D_{\mathrm{(play,lang)}}\}$ in this paper, this approach allows more generally for training over any set of imitation datasets with different descriptions&mdash;e.g. task id, language, human video demonstration, speech, etc.
Being context-agnostic enables a highly efficient training scheme: learn the majority of control from the cheapest data source, while learning the most general form of task conditioning from a small number of labeled examples. In this way, multicontext learning can be interpreted as transfer learning through a shared goal space. We exploit this strategy in this work. Crucially, this can reduce the cost of human oversight to the point where it can be practically applied. Multicontext learning allows us to train an agent to follow human instructions with less than 1% of collected robot experience requiring paired language, with the majority of control learned instead from relabeled goal image data.
<p/>

<div class="l-page-outset" id="lfp_vs_langlfp">
<figure>
<img id="fig:lfp_vs_langlfp" src="figs/training/lfp_vs_langlfp.png" style="margin: 0; width: 200%;"/>
  <figcaption>
  <b>Figure 3: Comparing original goal image conditioned LfP (left) to goal image or natural language conditioned LangLfP (right)</b>: Both are trained on top of teleoperated play, relabeled into millions of goal image conditioned imitation examples. LangLfP is additionally trained on play windows paired with hindsight instructions. This multicontext training allows for the majority of control to be learned from self-supervision (>99% relabeled play), with <1% of play windows requiring language pairing.
  </figcaption>
</figure>
</div>

<h4 id="sec:lang_lfp">4.3 LangLfP: following image and language goals.</h4>
<p>
We now have all the components to introduce <i>LangLfP</i> (language conditioned learning from play). LangLfP is a special case of multicontext imitation learning <a href="#sec:mcil">(Section 4.2)</a> applied to our problem setting.
At a high level, LangLfP trains a single multicontext policy $\pi_\theta(a_t | s_t, z)$ over datasets $\mathcal{D} = \{\DPLAY, \DPLAYLANG\}$, consisting of hindsight goal image tasks and hindsight instruction tasks. We define $\mathcal{F} = \{g_{\textrm{enc}}, s_{\textrm{enc}}\}$, neural network encoders mapping from image goals and instructions respectively to the same latent <i>visuo-lingual goal</i> space. LangLfP learns perception, natural language understanding, and control end-to-end with no auxiliary losses. We discuss the separate modules below.
</p>

<p>
<b>Perception module</b>. In our experiments, $\tau$ in each example consists of $\{(O_t, a_t)\}_t^{|\tau|}$, a sequence of onboard observations $O_t$ and actions. Each observation contains a high-dimensional image and internal proprioceptive sensor reading. A learned perception module $P_{\theta}$ maps each observation tuple to a low-dimensional embedding, e.g. $s_t = P_{\theta}(O_t)$, fed to the rest of the network. See <a href="#sec:appendix_perception">Appendix B.1</a> for details on our perception architecture. This perception module is shared with $g_{\textrm{enc}}$, which defines an additional network on top to map encoded goal observation $s_g$ to a point in $z$ space. See <a href="#sec:appendix_genc">Appendix B.2</a> for details.
</p>

<p>
<b>Language module</b>.
Our language goal encoder $s_{\textrm{enc}}$ tokenizes raw text $l$ into subwords <d-cite key="sennrich2015neural"></d-cite>, retrieves subword embeddings from a lookup table, then summarizes embeddings into a point in $z$ space. Subword embeddings are randomly initialized at the beginning of training and learned end-to-end by the final imitation loss. See <a href="#sec:appendix_nlu">Appendix B.3</a> for $s_{\textrm{enc}}$ implementation details.
</p>

<p>
<b>Control module</b>.
While we could in principle use any architecture to implement the multicontext policy $\pi_\theta(a_t | s_t, z)$, we use Latent Motor Plans (LMP) <d-cite key="lynch2019play"></d-cite>. LMP is a goal-directed imitation architecture that uses latent variables to model the large amount of multimodality inherent to freeform imitation datasets. Concretely, it is a sequence-to-sequence conditional variational autoencoder (seq2seq CVAE) autoencoding contextual demonstrations through a latent "plan" space. The decoder is a goal conditioned policy. As a CVAE, LMP lower bounds maximum likelihood contextual imitation (<a href="#eq:gcbc">Equation 1</a>), and is easily adapted to our multicontext setting. See <a href="#sec:appendix_control">Appendix B.4</a> for details on this module.
</p>

<p>
<b>LangLfP training</b>.
<a href="#fig:lfp_vs_langlfp">Figure 3</a> compares LangLfP training to original LfP training. At each training step we sample a batch of image goal tasks from $D_{\mathrm{play}}$ and a batch of language goal tasks from $D_{\mathrm{(play,lang)}}$. Observations are encoded into state space using the perception module $P_{\theta}$. Image and language goals are encoded into latent goal space $z$ using encoders $g_{\textrm{enc}}$ and $s_{\textrm{enc}}$. We then use $\pi_\theta(a_t | s_t, z)$ to compute the multicontext imitation objective, averaged over both task descriptions. We take a combined gradient step with respect to all modules&mdash;perception, language, and control&mdash;optimizing the whole architecture end-to-end as a single neural network. See a details in <a href="#sec:appendix_training">Appendix B.5</a>.
</p>

<p>
<b>Following human instructions at test time</b>.
At the beginning of a test episode the agent receives as input its onboard observation $O_t$ and a human-specified natural language goal $l$. The agent encodes $l$ in latent goal space $z$ using the trained sentence encoder $s_{\textrm{enc}}$. The agent then solves for the goal in closed loop, repeatedly feeding the current observation and goal to the learned policy $\pi_{\theta}(a|s_t, z)$, sampling actions, and executing them in the environment. The human operator can type a new language goal $l$ at any time. See picture in part 3 of <a href="#teaser">Figure 1</a>.
</p>

<h4 id="sec:transfer_lang_lfp">4.4 Transferring knowledge from generic text corpora.</h4>
<p>
Large "in the wild" natural language corpora reflect substantial human knowledge about the world <d-cite key="zellers2018swag"></d-cite>. Many recent works have successfully transferred this knowledge to downstream tasks in NLP via pretrained embeddings <d-cite key="devlin2018bert,radford2019language,raffel2019exploring,1901.05287"></d-cite>. Can we achieve similar knowledge transfer to <i>robotic manipulation</i>?
</p>

<div class="l-body" id="transfer">
<figure>
<img id="fig:transfer" src="figs/transfer/transfer.png" style="margin: 0; width: 80%;"/>
  <figcaption>
  <b>Figure 4. Following novel instructions at test time in zero shot using transfer learning:</b> Simply by training on top of pretrained language embeddings, we can give a language conditioned policy the ability to follow out of distribution instructions in zero shot at test time. The pretrained embedding space is responsible for relating novel instructions (green) to ones the agent has been trained to follow (black).
  </figcaption>
</figure>
</div>

<p>
We hypothesize two benefits to this type of transfer. First, if there is a semantic match between the source corpora and the target environment, more structured inputs may act as a strong prior, shaping grounding or control <d-cite key="luketina2019survey"></d-cite>. Second, language embeddings have been shown to encode similarity between large numbers of words and sentences. This may allow an agent to follow many novel instructions in zero shot, provided they are sufficiently "close" to ones it has been trained to follow (<a href="#fig:transfer">Figure 4</a>).
Note, given the complexity of natural language, it is likely that robots in open-world environments will need to be able to follow synonym commands outside of a particular training set.
</p>

<p>
To test these hypotheses, we introduce a simple transfer learning technique, generally applicable to any natural language conditioned agent. We assume access to a neural language model, pretrained on large unlabeled text corpora, capable of mapping full sentences $l$ to points in a semantic vector space $v \in \mathbb{R}^d$. Transfer is enabled simply by encoding language inputs $l$ to the policy at training and test time in $v$ before feeding to the rest of the network. We augment LangLfP in this way, which we refer to as <i>TransferLangLfP</i>. See Appendix<a href="#sec:appendix_nlu">Appendix B.3</a> for details.
</p>

<h3 id="experimental_setup">5. Experimental Setup</h3>
Our experiments aim to answer the following questions:
<ol>
  <li>"Ask me anything" scenario: Can LangLfP train a single agent to solve many human language conditioned robotic manipulation tasks in a row? How does LangLfP compare to prior goal image conditioned LfP, which has less practical task specification? How does our model, trained on unlabeled play, compare to an identical architecture trained on conventional labeled multitask demonstrations?
  </li>
  <li>Transfer from unlabeled text corpora: Can knowledge be transferred from text to downstream robotic manipulation? Does our transfer learning technique allow a policy to follow novel instructions in zero shot?
  </li>
</ol>

<div class="l-body" id="env">
<figure>
<img id="fig:env" src="figs/data/env.png" style="margin: 0; width: 80%;"/>
  <figcaption>
  <b>Figure 5. Playroom environment:</b> A situated robot in a 3D Mujoco environment.
  </figcaption>
</figure>
</div>

<h4 id="sec:sim_env">5.1 3D simulated environment</h4>
<p>
We conduct our experiments in the simulated 3D Playroom environment introduced in <d-cite key="lynch2019play"></d-cite>, shown in <a href="#fig:env">Figure 5</a>. The environment contains a desk with a sliding door and drawer that can be opened and closed. On the desk is a movable rectangular block and 3 buttons that can be pushed to control different colored lights. Next to the desk is a trash bin. Physics are simulated using the MuJoCo physics engine <d-cite key="TodorovET12"></d-cite>. Videos <a href="https://learning-from-play.github.io/assets/mp4/play_data516x360.mp4">here</a> of play data collected in this environment give a good idea of the available complexity. In front of the desk is a realistic simulated 8-DOF robot (arm and parallel gripper). The agent perceives its surroundings from egocentric high-dimensional RGB video sensors. It additionally has access to continuous internal proprioceptive sensors, relaying the cartesian position and rotation of its end effector and gripper angles. We modify the environment to include a text channel which the agent observes at each timestep, allowing humans to type unconstrained language commands. The agent must perform high-frequency, closed-loop continuous control to solve for user-described manipulation tasks, sending continuous position, rotation, and gripper commands to its arm at 30hz. See <a href="#sec:appendix_env">Appendix C</a> for details.
</p>

<h4 id="sec:methods">5.2 Methods</h4>
<p>
In our experiments, we compare the following methods (more details in <a href="#sec:appendix_models">Appendix E</a>):
<ul>
  <li>
    <b>LangBC</b> ("language, but no play"): a baseline natural language conditioned multitask imitation policy <d-cite key="rahmatizadeh2018vision"></d-cite>, trained on $D_{\mathrm{(play,lang)}}$, 100 expert demonstrations for each of the 18 evaluation tasks paired with hindsight instructions.
  </li>
  <li>
    <b>LfP</b>  ("play, but no language"): a baseline LfP model trained on $D_{\mathrm{play}}$, conditioned on goal images at test time.
  </li>
  <li>
    <b>LangLfP (ours)</b>  ("play and language"): Multicontext imitation trained on $D_{\mathrm{play}}$ and play paired with language $D_{\mathrm{(play,lang)}}$. Tasks are specified at test time using only natural language.
  </li>
  <li>
    <b>Restricted LangLfP</b>  To make a controlled comparison to LangBC, we train a baseline of LangLfP on "restricted $D_{\mathrm{play}}$", a play dataset restricted to be the same size as $D_{\mathrm{(play,lang)}}$. This restriction is artificial, as more unsegmented play can be collected with the same budget of time. This helps answer "for the same exact amount of data, which kind of data leads to better generalization: conventional multitask demonstrations, or relabeled play?".
  </li>
  <li>
    <b>TransferLangLfP (ours)</b>: transfer learning augmented LangLfP, identical except instruction inputs are replaced with pretrained language embeddings.
  </li>
</ul>
</p>

<p>
We define two sets of experiments for each baseline: pixel experiments, where models receive high-dimensional image observations and must learn perception end-to-end, and state experiments, where models instead receive the ground truth simulator state consisting of positions and orientations for all objects in the scene. The latter provides an upper bound on how all well the various methods can learn language conditioned control, independent of a difficult perception problem (which may be improved upon independently with self-supervised representation learning methods (e.g. <d-cite key="{Sermanet2017TCN,oord2018representation,ozair2019wasserstein,pirk2019online"></d-cite>).
</p>

<p>
Note that all baselines can be seen as maximizing the same multicontext imitation objective, differing only on the particular composition of data sources. Therefore, to perform a controlled comparison, we use the same imitation architecture (details in <a href="#appendix_architecture">Appendix B</a>) across all baselines. See <a href="#sec:appendix_dataset">Appendix D</a> for a detailed description of all data sources.
</p>

<!-- COREY ABOVE THIS LINE -->

<h3 id="ama">6. "Ask Me Anything" Experiments</h3>
<p>
While one-off multitask evaluations <d-cite key="yu2019meta, lynch2019play"></d-cite> are challenging and interesting, a realistic and significantly more difficult scenario is one in which a human gives a robot multiple instructions in a row over a <i>long horizon</i>, for example: "get the block from the shelf...now open the drawer...now sweep the block into the drawer...now close the drawer". Furthermore, agents should be able to accomplish any subtask in any order. This demands a training process that adequately covers transitions between arbitrary pairs of tasks. This setting of long-horizon human language conditioned robotic manipulation has not yet been explored in the literature to our knowledge. While difficult, we believe it aligns well with test-time expectations for a learning robot in everyday environments.
</p>

<h4 id="sec:long_horizon_eval">6.1 Long-Horizon Evaluation</h4>
<p>
We define a long horizon evaluation by significantly expanding upon the previous 18-task evaluation in <d-cite key="lynch2019play"></d-cite>, Multi-18. We construct many multi-stage evaluations by treating the original 18 tasks as subtasks, then consider all valid N-stage transitions between them. This results in 2-stage, 3-stage, and 4-stage long horizon manipulation benchmarks, referred to here as Chain-2, Chain-3, and Chain-4. Note that the number of unique tasks increases rapidly with N: there are 63 Chain-2 tasks, 254 Chain-3 tasks, and 925 Chain-4 tasks. Also note this multi-stage scenario subsumes the original Multi-18, which can be seen as just testing the first stage. This allows for direct comparison to prior work. See <a href="#sec:appendix_eval">Appendix F</a> for details on benchmark construction and evaluation walkthrough. We evaluate all methods on these long-horizon benchmarks and present the results in <a href="#tab:main_results">Table 1</a> and <a href="#fig:long_horizon">Figure 6</a>, discussing our findings below. Success is reported with confidence intervals over 3 seeded training runs.
</p>

<div class="l-body-outset" id="env">
<figure>
<img id="tab:main_results" src="figs/tables/table1.png" style="margin: 0; width: 100%;"/>
  <figcaption>
  <b>Table 1. "Ask Me Anything" experiments:</b> long horizon, natural language conditioned visual manipulation with a human in the loop. We report the percentage of subtasks completed for one-off instructions (Multi-18) and four instructions in a row (Chain-4). We see that LangLfP matches the performance of LfP, but has a more scalable form of task conditioning. Additionally we see TransferLangLfP significantly benefits from knowledge transfer from generic text corpora, outperforming LangLfP and original LfP. Finally, models trained on unstructured play significantly outperform LangBC, trained on predefined task demonstrations, especially in the long horizon.
  </figcaption>
</figure>
</div>

<div class="l-body" id="env">
<figure>
<img id="fig:long_horizon" src="figs/results/fig6.png" style="margin: 0; width: 90%;"/>
  <figcaption>
  <b>Figure 6. Multitask human language conditioned visual manipulation over a long horizon.</b> Here we plot the performance of all methods as a human gives a sequence of natural language instructions to follow. Note there are 18 single instruction tasks, rising to 925 unique four instruction tasks. The x-axis considers the number of instructions given in a row and the y-axis considers the percentage of instructions successfully completed. We also plot goal image conditioned LfP performance for comparison.
  </figcaption>
</figure>
</div>

<p id="qualitative-langlfp"><b>Video 4: Test time human language conditioned visual control</b>. Here are examples of LangLfP following human instructions at test time from onboard observations. See more qualitative examples in <a href="#sec:qualitative_examples">Appendix G</a>.</p>
<figure>
<video class="b-lazy" loop autoplay playsinline muted style="display: block; width: 88%; margin:0 auto" data-src="videos/pixels/long_chains/long_chains_drawer-sweep-grasp_lift-close_drawer_iter96001_chain_demo0_lang-en_trial16_success1.0.mp4_frames_bt300k.mp4" type="video/mp4"></video>
<!-- <figcaption>
  LangLfP solves natural language specified tasks from onboard observations.
</figcaption> -->
</figure>

<figure>
<video class="b-lazy" loop autoplay playsinline muted style="display: block; width: 88%; margin:0 auto" data-src="videos/pixels/long_chains/long_chains_put_in_shelf-pull_out_shelf-put_in_shelf-pull_out_shelf-grasp_flat_iter64001_chain_demo0_lang-en_trial22_success1.0.mp4_frames_bt300k.mp4" type="video/mp4"></video>
<!-- <figcaption>
  LangLfP solves natural language specified tasks from onboard observations.
</figcaption> -->
</figure>

<figure>
<video class="b-lazy" loop autoplay playsinline muted style="display: block; width: 88%; margin:0 auto" data-src="videos/pixels/long_chains/long_chains_drawer-sweep-close_drawer-sliding-push_green-push_blue-push_red-close_sliding-drawer-close_drawer-sliding-drawer_iter128001_chain_demo0_lang-en_trial29_success1.0.mp4_frames_bt300k.mp4" type="video/mp4"></video>
<!-- <figcaption>
  LangLfP solves natural language specified tasks from onboard observations.
</figcaption> -->
</figure>

<h4 id="sec:long_horizon_results">6.2 Long Horizon Results</h4>
<p>
<b>Goal image conditioned comparison.</b>
In <a href="#tab:main_results">Table 1</a>, we find that LangLfP matches the performance of LfP on all benchmarks within margin of error, but does so with entirely from natural language task specification. Notably, this is achieved with only $\thicksim$0.1% of play requiring language pairing, with the majority of control still learned from self-supervised imitation. This is important because it shows our simple framework allows open-ended robotic manipulation to be combined with open-ended text conditioning with minimal additional human supervision. This was the main question of this work. We attribute these results to our multicontext learning setup, which allows statistical strength to be shared over multiple datasets, some plentiful and some scarce. See <a href="#qualitative-langlfp">Video 4</a> for examples of a single LangLfP agent following many natural language instructions from onboard observations.
</p>

<p><b>Conventional multitask imitation comparison.</b>
We see in <a href="#tab:main_results">Table 1</a> and <a href="#fig:long_horizon">Figure 6</a> that LangLfP outperforms LangBC on every benchmark. Notably, this result holds even when the play dataset is artificially restricted to the same size as the conventional demonstration dataset (Restricted LangLfP vs LangBC). This is important because it shows that models trained on top of relabeled play generalize much better than those trained on narrow predefined task demonstrations, especially in the most difficult long horizon setting.
Qualitatively (\href{\website/\#lfp-vs-bc}{videos}), the difference between the training sources is striking. We find that play-trained policies can of transition well between tasks and recover from initial failures, while narrow demo-trained policies quickly encounter compounding imitation error and never recover. We believe this highlights the importance of training data that covers state space, including large numbers of tasks as well as non task-specific behavior like transitions.
</p>

<p id="lfp-vs-bc"><b>Video 6: Play vs. conventional demonstrations</b>. Here we qualitatively compare LangLfP to LangBC by providing each with the exact same set of 4 instructions.</p>

<div class="l-page-outset" id="lfpvsbc_table">
<table border="0" cellpadding="5" style="text-align: center">
  <tr>
    <td><big><b>LangLfP</b></big></td>
    <td><big><b>LangBC</b></big></td>
  </tr>
  <tr>
    <td>
      <figure>
      <video class="b-lazy" loop autoplay playsinline muted style="display: block; width: 100%; margin:0 auto" data-src="videos/chain4/chain4_lfp_drawer-sweep-close_drawer-sliding_bt300k.mp4" type="video/mp4"></video>
      </figure>
    </td>
    <td>
      <figure>
      <video class="b-lazy" loop autoplay playsinline muted style="display: block; width: 100%; margin:0 auto" data-src="videos/chain4/chain4_bc_drawer-sweep-close_drawer-sliding_bt300k.mp4" type="video/mp4"></video>
      </figure>
    </td>
  </tr>
  <tr>
     <td colspan=2><small>
     Instructions:
      1) "pull the drawer handle all the way"
      2) "put the block in to the drawer"
      3) "push the drawer in"
      4) "move the door all the way right"
      <br><br></small>
    </td>
  </tr>
 
  

  <tr>
    <td><big><b>LangLfP</b></big></td>
    <td><big><b>LangBC</b></big></td>
  </tr>
  <tr>
    <td>
      <figure>
      <video class="b-lazy" loop autoplay playsinline muted style="display: block; width: 100%; margin:0 auto" data-src="videos/chain4/chain4_lfp_pull_out_shelf-sliding-push_blue-close_sliding_bt300k.mp4" type="video/mp4"></video>
      </figure>
    </td>
    <td>
      <figure>
      <video class="b-lazy" loop autoplay playsinline muted style="display: block; width: 100%; margin:0 auto" data-src="videos/chain4/chain4_bc_pull_out_shelf-sliding-push_blue-close_sliding_bt300k.mp4" type="video/mp4"></video>
      </figure>
    </td>
  </tr>
  <tr>
     <td colspan=2><small>
     Instructions:
      1) "drag the block out"
      2) "push the door to the right"
      3) "press blue"
      4) "move the door all the way left"
      <br><br></small>
    </td>
  </tr>

  <tr>
    <td><big><b>LangLfP</b></big></td>
    <td><big><b>LangBC</b></big></td>
  </tr>
  <tr>
    <td>
      <figure>
      <video class="b-lazy" loop autoplay playsinline muted style="display: block; width: 100%; margin:0 auto" data-src="videos/chain4/chain4_lfp_knock-drawer-sweep-close_drawer_bt300k.mp4" type="video/mp4"></video>
      </figure>
    </td>
    <td>
      <figure>
      <video class="b-lazy" loop autoplay playsinline muted style="display: block; width: 100%; margin:0 auto" data-src="videos/chain4/chain4_bc_knock-drawer-sweep-close_drawer_bt300k.mp4" type="video/mp4"></video>
      </figure>
    </td>
  </tr>
  <tr>
     <td colspan=2><small>
     Instructions:
      1) "knock the object"
      2) "pull the drawer handle all the way"
      3) "drag the object into the drawer"
      4) "close the drawer"
      <br><br></small>
    </td>
  </tr>
  
</table>
</div>


<p id="15tasks"><b>Following 15 instructions in a row.</b> We present qualitative results in <a href="#long-horizon">Video 5</a> showing that our agent can follow 15 natural language instructions in a row provided by a human. We believe success in this difficult scenario offers encouragement that simple high capacity imitation learning can be competitive with more complicated long horizon reinforcement learning methods.
</p>

<figure>
  <video id="long-horizon" class="b-lazy" loop autoplay playsinline muted style="display: block; width: 70%; margin:0 auto" data-src="videos/long/chain15_iter52001_chain_demo0_lang-en_trial4_success1.0.mp4_frames_bt300k.mp4" type="video/mp4">
      Your browser does not support the video tag.
  </video>
  <figcaption>
  <b>Video 5. LangLfP follows 15 instructions in a row.</b>
  </figcaption>
</figure>

<p><b>Play scales with model capacity.</b>
In <a href="#fig:sweep_capacity">Figure 7</a>, we consider task performance as a function of model size for models trained on play or conventional predefined demonstrations. For fair comparison, we compare
LangBC to Restricted LangLfP, trained on the same amount of data. We study both models from state inputs to understand upper bound performance. As we see, performance steadily improves as model size is increased for models trained on play, but peaks and then declines for models trained on conventional demonstrations. Our interpretation is that larger capacity is effectively utilized to absorb the diversity in play, whereas additional capacity is wasted on equivalently sized datasets constrained to predefined behaviors. This suggests that the simple recipe of collecting large play datasets and scaling up model capacity to achieve higher performance is a valid one.
</p>

<div class="l-body" id="env">
<figure>
<img id="fig:sweep_capacity" src="figs/results/fig7_capacity.png" style="margin: 0; width: 80%;"/>
  <figcaption>
  <b>Figure 7. Play scales with model capacity.</b> Large model capacity is well utilized to absorb diversity in unstructured play, but is wasted on an equivalent amount of conventional predefined task demonstrations.
  </figcaption>
</figure>
</div>

<p id="human_assistance"><b>Language unlocks human assistance.</b>
Natural language conditioned control allows for new modes of interactive test time behavior, allowing humans to give guidance to agents at test time that would be impractical to give via goal image or task id conditioned control. See <a href="#interactive">Video 7</a> for a concrete example. During the long horizon evaluation, the arm of a LangLfP agent becomes stuck on top of the desk shelf. The operator is able to quickly specify a new subtask "pull your arm back", which the agent completes, leaving it in a much better initialization to complete the original task. This rapid interactive guidance would have required having a corresponding goal image on hand for "pull your arm back" in the image conditioned scenario or would have required learning an entire separate task using demonstrations in the task id conditioned scenario.
</p>

<figure>
<video id="interactive" class="b-lazy" loop autoplay playsinline muted style="display: block; width: 70%; margin:0 auto" data-src="videos/live/playlang_20200326-191101_interaction_bt300k.mp4" type="video/mp4"></video>
<figcaption>
<b>Video 7: Interactive human language assistance: </b>The operator offers real time language guidance to the robot whose end-effector gets stuck, allowing it to solve for "press the red button".
</figcaption>
</figure>


<p id="ood_tasks"><b>Video 8: Composing new tasks with language</b>.
In this section, we show that the operator can compose difficult new tasks on the fly with language, e.g. <a href="#fig:trash">"put the object in the trash"</a> or <a href="#fig:top_of_shelf">"put the object on top of the shelf"</a>. These tasks are outside the standard 18-task benchmark. LangLfP solves them in zero shot. Note that the operator helps the robot by breaking down the task into subtasks.
</p>

<div class="l-page-outset" id="ood_tasks_table">
<table border="0" cellpadding="5" style="text-align: center">
  <tr>
    <td>
      <figure>
      <video id="fig:trash" class="b-lazy" loop autoplay playsinline muted style="display: block; width: 100%; margin:0 auto" data-src="videos/live/playlang_20200325-154140_trash_bt300k.mp4" type="video/mp4"></video>
      <figcaption><b>Putting the object in the trash:</b> An operator is able to put the object in the trash by breaking down the task into 2 smaller subtasks with the following sentences; 1) "pick up the object" 2) "put the object in the trash".</figcaption>
      </figure>
    </td>
    <td>
      <figure>
      <video id="fig:top_of_shelf" class="b-lazy" loop autoplay playsinline muted style="display: block; width: 100%; margin:0 auto" data-src="videos/live/playlang_20200325-162306_top_shelf_bt300k.mp4" type="video/mp4"></video>
      <figcaption><b>Putting the object on top of the shelf:</b> An operator is able to put the object in the trash by breaking down the task into 2 smaller subtasks with the following sentences; 1) "pick up the object" 2) "put the object on top of the shelf".</figcaption>
      </figure>
    </td>
  </tr>
</table>
</div>

<h3 id="transfer">7. Knowledge Transfer Experiments</h3>
<p>
Here we evaluate our transfer learning augmentation to LangLfP. In these experiments, we are interested in two questions: 1) Is knowledge transfer possible from generic text corpora to language-guided robotic manipulation? 2) Does training on top of pretrained embeddings allow our policy to follow instructions it has never been trained on?
</p>

<div class="l-body" id="env">
<figure>
<img id="fig:systematic_transfer" src="figs/results/fig8_knowledge_transfer.png" style="margin: 0; width: 90%;"/>
  <figcaption>
  <b>Figure 8. Knowledge transfer from generic text corpora to robotic manipulation.</b>
  TransferLangLfP, which trains on top of pretrained language embeddings, systematically outperforms LangLfP, which learns language understanding from scratch, on all manipulation benchmarks.
  </figcaption>
</figure>
</div>

<div class="l-body" id="env">
<figure>
<img id="tab:ood" src="figs/tables/table2_ood.png" style="margin: 0; width: 60%;"/>
  <figcaption>
  <b>Table 2. Following out of distribution instructions:</b> Training on top of pretrained neural embeddings (TransferLangLfP) allows our agent to follow thousands of out-of-distribution synonym instructions in 16 languages in zero shot.
  </figcaption>
</figure>
</div>

<h4>7.1 Knowledge Transfer Results</h4>
<p><b>Positive transfer to robotic manipulation.</b>
In <a href="#tab:main_results">Table 1</a> and <a href="#fig:systematic_transfer">Figure 8</a>, we see that while LangLfP and prior LfP perform comparably, TransferLangLfP systematically outperforms both. This is important because it shows the first evidence, to our knowledge, that world knowledge reflected in large unstructured bodies of text can be transferred downstream to improve language-guided robotic manipulation. As mentioned in <d-cite key="luketina2019survey"></d-cite>, we hypothesize this transfer is possible because we conduct experiments in a 3D environment with realistic object interactions, matching the semantics of those described in real world textual corpora.
</p>
<p><b>Following out of distribution "synonym instructions"</b>. To study whether our proposed transfer augmentation allows an agent to follow out-of-distribution instructions, we replace one or more words in each Multi-18 instruction with a synonym outside training, e.g. "drag the block from the shelf" $\xrightarrow{}$ "<i>retrieve</i> the <i>brick</i> from the <i>cupboard</i>". Enumerating all substitutions results in a set of 14,609 out-of-distribution instructions covering all 18 tasks. We evaluate a random policy, LangLfP, and TransferLangLfP on this benchmark, OOD-syn, reporting the results in <a href="#tab:ood">Table 2</a>. Success is reported with confidence intervals over 3 seeded training runs. We see while LangLfP is able to solve some novel tasks by inferring meaning from context (e.g. "pick up the block" and "pick up the brick" might reasonably map to the same task), its performance degrades significantly. TransferLangLfP on the other hand, generalizes substantially better. This shows that the simple transfer learning technique we propose greatly magnifies the test time scope of an instruction following agent, allowing it to follow thousands more user instructions than it was trained with. Given the inherent complexity of language, this is an important real world consideration.
</p>
<p><b>Following out of distribution instructions in 16 different languages.</b> Here we study a rather extreme case of out-of-distribution generalization: following instructions in languages not seen during training (e.g. French, Mandarin, German, etc.).
To study this, we combine the original set of test instructions from Multi-18 with the expanded synonym instruction set OOD-syn, then translate both into 16 languages using the Google translate API. This results in $\thicksim$240k out-of-distribution instructions covering all 18 tasks. We evaluate the previous methods on this cross-lingual manipulation benchmark, OOD-16-lang, reporting success in <a href="#tab:ood">Table 2</a>. We see that when LangLfP receives instructions with no training overlap, it resorts to producing <a href="#transferlanglfp_vs_langlfp">maximum likelihood play actions</a>. This results in some spurious task success, but performance degrades materially. Remarkably, TransferLangLfP solves a substantial portion of these in zero shot, degrading only slightly from the english-only benchmark. See \href{\website/\#multilingual}{videos} of TransferLangLfP following instructions in 16 novel languages. These results show that simply training high capacity imitation policies on top of pretrained embedding spaces affords agents powerful zero shot generalization capabilities. While practically, one could imagine simply translating these instructions first into English before feeding them to our system, this demonstrates that a large portion of the necessary analogical reasoning can happen internal to the agent in a manner that is end-to-end differentiable. While in this paper we do not finetune the embeddings in this manner, an exciting direction for future work would be to see if grounding language embeddings in embodied imitation improves representation learning over text-only inputs.
</p>

<p id="multilingual"><b>Video 9: Transfer unlocks zero shot instruction following</b></p>

<!-- <\!-- <div class="l-body" id="demo"> -\-> -->
<!-- <figure> -->

  <video class="b-lazy" loop autoplay playsinline muted style="display: block; width: 80%; margin:0 auto"
         data-src="videos/languages/multilingual_bt300k.mp4" type="video/mp4">
    Your browser does not support the video tag.
  </video>
<figcaption>
<b>Following instructions in 16 different languages</b>
</figcaption>
</figure>
<!-- </div> -->

<!-- <div class="l-body" id="demo"> -->
<figure>

  <video class="b-lazy" loop autoplay playsinline muted style="display: block; width: 80%; margin:0 auto"
         data-src="videos/languages/playlang_20200327-141234_fr_bin_bt300k.mp4" type="video/mp4">
    Your browser does not support the video tag.
  </video>
<figcaption>
<b>Real-time instructions in French, even though agent only trained with English.</b> 6 instructions: "open the drawer... put the object into the drawer...take the object out of the drawer...close the drawer...grasp the object...put the object in the bin."
</figcaption>
</figure>


<h3 id="future">8. Limitations and Future Work</h3>
<p>
Although the coverage of play mitigates failure modes in conventional imitation setups, we observe several limitations in our policies at test time.
In this <a href="#failure1">video</a>, we see the policy make multiple attempts to solve the task, but times out before it is able to do so. We see in this <a href="#failure2">video</a> that the agent encounters a particular kind of compounding error, where the arm flips into an awkward configuration, likely avoided by humans during teleoperated play. This is potentially mitigated by a more stable choice of rotation representation, or more varied play collection. We note that the human is free to help the agent out of these awkward configurations using language assistance, as shown in <a href="#interactive">Video 7</a>. More examples of failures can be seen <a href="#failures">here</a>.
</p>

<figure id="failure1">
<video class="b-lazy" loop autoplay playsinline muted style="display: block; width: 88%; margin:0 auto" data-src="videos/pixels/failures/failures_knock-grasp_flat_iter182001_chain_demo0_lang-en_trial3_success0.5.mp4_frames_bt300k.mp4" type="video/mp4"></video>
<figcaption>
Failure: The agent times out attempting to lift the block.
</figcaption>
</figure>

<figure id="failure2">
<video class="b-lazy" loop autoplay playsinline muted style="display: block; width: 88%; margin:0 auto" data-src="videos/pixels/failures/failures_put_in_shelf-sliding_iter182001_chain_demo1_lang-en_trial2_success0.0.mp4_frames_bt300k.mp4" type="video/mp4"></video>
<figcaption>
Failure: The agent encounters an arm configuration likely avoided by human operators during teleoperated play, leading to compounding error.
</figcaption>
</figure>


<p>
While LangLfP relaxes important constraints around task specification, it is fundamentally a goal-directed imitation method and lacks a mechanism for autonomous policy improvement. An exciting area for future work may be one that combines the coverage of teleoperated play, the scalability and flexibility of multicontext imitation pretraining, and the autonomous improvement of reinforcement learning, similar to prior successful combinations of LfP and RL <d-cite key="gupta2019relay"></d-cite>.
</p>

<p>
As in original LfP, the scope of this work is task-agnostic control in a single environment. We note this is consistent with the standard imitation assumptions that training and test tasks are drawn i.i.d. from the same distribution. An interesting question for future work is whether training on a large play corpora covering many rooms and objects allows for generalization to unseen rooms or objects.
</p>

<p>
Simple maximum likelihood makes it easy to learn large capacity perception and control architectures end-to-end. It additionally enjoys significant sample efficiency and stability benefits over multitask RL at the moment <d-cite key="schaul2019ray"></d-cite>. We believe these properties provide good motivation for continuing to scale larger end-to-end imitation architectures over larger play datasets as a practical strategy for task-agnostic control.
</p>

<h3 id="conclusion">9. Conclusion</h3>
<p>
Robots that can learn to relate human language to their perception and actions would be especially useful in open-world environments.
This work asks: provided real humans play a direct role in this learning process, what is the most efficient way to go about it?
With this motivation, we introduced LangLfP, an extension of LfP trained both on relabeled goal image play and play paired with human language instructions. To reduce the cost of language pairing, we introduced Multicontext Imitation Learning, which allows a single policy to be trained over both goal image and language tasks, then use just language conditioning at test time.
Crucially, this made it so that less than  1% collected robot experience required language pairing to enable the new mode of conditioning. In experiments, we found that a single policy trained with LangLfP can solve many 3D robotic manipulation tasks over a long horizon, from onboard sensors, and specified only via human language. This represents a considerably more complex scenario than prior work in instruction following. Finally, we introduced a simple technique allowing for knowledge transfer from large unlabeled text corpora to robotic manipulation. We found that this significantly improved downstream visual control&mdash;the first instance to our knowledge of this kind of transfer. It also equipped our agent with the ability to follow thousands of instructions outside its training set in zero shot, in 16 different languages.
</p>

<!-- appendix -------------------------------------------------------------------------------------------------------------------- -->

<h2 id="appendix">Appendix</h2>
<h3 id="relabeling">A. Relabeling play</h3>

<div class="l-body" id="alg2">
<figure>
<img id="alg2" src="algs/alg2.png" style="margin: 0; width: 70%;"/>
  <figcaption>
  <b>Algorithm 2: Relabeling play into many goal image demonstrations.</b>
  </figcaption>
</figure>
</div>

<div class="l-body" id="alg3">
<figure>
<img id="alg3" src="algs/alg3.png" style="margin: 0; width: 70%;"/>
  <figcaption>
  <b>Algorithm 3: Scalably pairing robot experience with relevant human language.</b>
  </figcaption>
</figure>
</div>

<h3 id="sec:appendix_architecture">B. LangLfP Implementation Details</h3>
<p>
Below we describe the networks we use for perception, natural language understanding, and control&mdash;all trained end-to-end as a single neural network to maximize our multicontext imitation objective. We stress that this is only one implementation of possibly many for LangLfP, which is a general high-level framework 1) combining relabeled play, 2) play paired with language, and 3) multicontext imitation.
</p>

<h4 id="sec:appendix_perception">B.1 Perception Module</h4>
<p>
We map raw observations (image and proprioception) to low dimensional perceptual embeddings that are fed to the rest of the network. To do this we use the perception module described in <a href="#fig:perception_sentence_emb">Figure 9</a>. We pass the image through the network described in <a href="#tab:vision_net_hparams">Table 3</a>, obtaining a 64-dimensional visual embedding. We then concatenate this with the proprioception observation (normalized to zero mean, unit variance using training statistics). This results in a combined perceptual embedding of size 72.
This perception module is trained end to end to maximize the final imitation loss. We do no photometric augmentation on the image inputs, but anticipate this may lead to better performance.
</p>

<div class="l-body" id="env">
<figure>
<img id="fig:perception_sentence_emb" src="figs/appendix/fig9_perception_sentence_emb.png" style="margin: 0; width: 100%;"/>
  <figcaption>
  <b>Figure 9. Perception and language embedder modules.</b>
  </figcaption>
</figure>
</div>

<div class="l-body" id="env">
<figure>
<img id="tab:vision_net_hparams" src="figs/tables/table3_hparams.png" style="margin: 0; width: 50%;"/>
  <figcaption>
  <b>Table 3. Hyperparameters for vision network.</b>
  </figcaption>
</figure>
</div>

<h4 id="sec:appendix_genc">B.2 Image goal encoder</h4>

<p>
$g_{\textrm{enc}}$ takes as input image goal observation $O_g$ and outputs latent goal $z$. To implement this, we reuse the perception module $P_{\theta}$, described in <a href="#sec:appendix_perception">Appendix B.1</a>, which maps $O_g$ to $s_g$. We then pass $s_g$ through a 2 layer 2048 unit ReLU MLP to obtain $z$.
</p>

<h4 id="sec:appendix_nlu">B.3 Language understanding module</h4>

<p><b>From scratch</b>. For LangLfP, our latent language goal encoder, $s_{\textrm{enc}}$, is described in <a href="#fig:perception_sentence_emb">Figure 9</a>. It maps raw text to a 32 dimensional embedding in goal space as follows: 1) apply subword tokenization, 2) retrieve learned 8-dim subword embeddings from a lookup table, 3) summarize the sequence of subword embeddings (we considered average pooling and RNN mechanisms for this, choosing average pooling based on validation), and 4) pass the summary through a 2 layer 2048 unit ReLU MLP. Out-of-distribution subwords are initialized as random embeddings at test time.
</p>
<p><b>Transfer learning.</b>
For our experiments, we chose Multilingual Universal Sentence Encoder described in <d-cite key="yang2019multilingual"></d-cite>. MUSE is a multitask language architecture trained generic multilingual corpora (e.g. Wikipedia, mined question-answer pair datasets), and has a vocabulary of 200,000 subwords. MUSE maps sentences of any length to a 512-dim vector. We simply treat these embeddings as language observations and do not finetune the weights of the model. This vector is fed to a 2 layer 2048 unit ReLU MLP, projecting to latent goal space.

We note there are many choices available for encoding raw text in a semantic pretrained vector space. MUSE showed results immediately and we moved onto different decisions. We look forward to experimenting with different choices of pretrained embedder in the future.
</p>

<h4 id="sec:appendix_control">B.4 Control Module</h4>

<p><b>Multicontext LMP</b>. Here we describe "multicontext LMP": LMP adapted to be able to take either image or language goals. This imitation architecture learns both an abstract visuo-lingual goal space $z^g$, as well as a plan space $z^p$ capturing the many ways to reach a particular goal. We describe this implementation now in detail.
</p>
<p>
As a conditional seq2seq VAE, original LMP trains 3 networks. 1) A posterior $q(z^p|\tau)$, mapping from full state-action demonstration $\tau$ to a distribution over recognized plans. 2) A learned prior $p(z^p|s_0, s_g)$, mapping from initial state in the demonstration and goal to a distribution over possible plans for reaching the goal. 3) A goal and plan conditioned policy $p(a_t|s_t, s_g, z^p)$, decoding the recognized plan with teacher forcing to reconstruct actions that reach the goal.
</p>
<p>
To train multicontext LMP, we simply replace the goal conditioning on $s_g$ everywhere with conditioning on $z^g$, the latent goal output by multicontext encoders $\mathcal{F}= \{g_{\textrm{enc}},s_{\textrm{enc}}\}$. In our experiments, $g_{\textrm{enc}}$ is a 2 layer 2048 unit ReLU MLP, mapping from encoded goal image $s_g$ to a 32 dimensional latent goal embedding. $s_{\textrm{enc}}$ is a subword embedding summarizer described in <a href="#sec:appendix_nlu">Appendix B.3</a>. Unless specified otherwise, the LMP implementation in this paper uses the same hyperparameters and network architecture as in <d-cite key="lynch2019play"></d-cite>. See appendix there for details on the posterior, conditional prior, and decoder architecture, consisting of a RNN, MLP, and RNN respectively.
</p>

<h4 id="sec:appendix_training">B.5 Training Details</h4>
<p>
At each training step, we compute two contextual imitation losses: image goal and language goal. The image goal forward pass is described in <a href="#fig:im_goal_lmp">Figure 16</a>. The language goal forward pass is described in <a href="#fig:lang_goal_lmp">Figure 17</a>. We share the perception network and LMP networks (posterior, prior, and policy) across both steps. We average minibatch gradients from image goal and language goal passes and we train everything&mdash;perception networks, $g_{\textrm{enc}}$, $s_{\textrm{enc}}$, posterior, prior, and policy&mdash;end-to-end as a single neural network to maximize the combined training objective. We describe all training hyperparameters in <a href="#tab:training_hparams">Table 4</a>.
</p>

<div class="l-body-outset" id="env">
<figure>
<img id="fig:im_goal_lmp" src="figs/appendix/fig16_imlfp.png" style="margin: 0; width: 100%;"/>
  <figcaption>
  <b>Figure 16. Latent image goal LMP:</b> This image goal conditioned forward pass of multicontext LMP.
  1) Map sequence of raw observations $O_t$ to perceptual embeddings (see <a href="#fig:perception_sentence_emb">Figure 9</a>). 2) Map image goal to latent goal space. 3) Map full state-action sequence to recognized plan through posterior. 4) Map initial state and latent goal through prior network to distribution over plans for achieving goal. 5) Minimize KL divergence between posterior and prior. 6) Compute maximum likelihood action reconstruction loss by decoding plan into actions with teacher forcing. Each action prediction is conditioned on current perceptual embedding, latent image goal, and latent plan. Note perception net, posterior, prior, and policy are shared with language goal forward pass.
  </figcaption>
</figure>
</div>

<div class="l-body-outset" id="env">
<figure>
<img id="fig:lang_goal_lmp" src="figs/appendix/fig17_langlfp.png" style="margin: 0; width: 100%;"/>
  <figcaption>
  <b>Figure 17. Latent language goal LMP:</b> This describes the language goal conditioned forward pass of multicontext LMP. 1) Map sequence of raw observations $O_t$ to perceptual embeddings (see <a href="#fig:perception_sentence_emb">Figure 9</a>). 2) Map language observation to latent goal space. 3) Map full state-action sequence to recognized plan through posterior. 4) Map initial state and latent goal through prior network to distribution over plans for achieving goal. 5) Minimize KL divergence between posterior and prior. 6) Compute maximum likelihood action reconstruction loss by decoding plan into actions with teacher forcing. Each action prediction is conditioned on current perceptual embedding, latent language goal, and latent plan. Note perception net, posterior, prior, and policy are shared with image goal LMP.
  </figcaption>
</figure>
</div>

<div class="l-body-outset" id="env">
<figure>
<img id="tab:training_hparams" src="figs/tables/table4_training_hparams.png" style="margin: 0; width: 100%;"/>
  <figcaption>
  <b>Table 4. Training Hyperparameters.</b>
  </figcaption>
</figure>
</div>

<h3 id="sec:appendix_env">C. Environment</h3>
<p>
We use the same simulated 3D playground environment as in <d-cite key="lynch2019play"></d-cite>, keeping the same observation and action spaces. We define these below for completeness.
</p>

<h4>C.1 Observation space</h4>
<p>
We consider two types of experiments: pixel and state experiments. In the pixel experiments, observations consist of (image, proprioception) pairs of 200x200x3 RGB images and 8-DOF internal proprioceptive state. Proprioceptive state consists of 3D cartesian position and 3D euler orientation of the end effector, as well as 2 gripper angles.
In the state experiments, observations consist of the 8D proprioceptive state, the position and euler angle
orientation of a movable block, and a continuous 1-d sensor describing: door open amount, drawer open amount, red
button pushed amount, blue button pushed amount, green button pushed amount. In both experiments, the agent additionally observes the raw string value of a natural language text channel at each timestep.
</p>

<h4>C.2 Action space</h4>
<p>
We use the same action space as <d-cite key="lynch2019play"></d-cite>: 8-DOF cartesian position, euler, and gripper angle of the end effector. Similarly, during training we quantize each action element into 256 bins. All stochastic policy outputs are represented as discretized logistic distributions over quantization bins <d-cite key="1701.05517"></d-cite>.
</p>

<h3 id="sec:appendix_dataset">D. Datasets</h3>

<h4>D.1 Play dataset</h4>
<p>
We use the same play logs collected in <d-cite key="lynch2019play"></d-cite> as the basis for all relabeled goal image conditioned learning. This consists of $\thicksim$7h of play relabeled in to $D_{\mathrm{play}}$: $\thicksim$10M short-shorizon windows, each spanning 1-2 seconds.
</p>

<h4>D.2 (Play, language) dataset</h4>

<p>
We pair 10K windows from $D_{\mathrm{play}}$ with natural language hindsight instructions using the interface shown in <a href="#fig:ui">Figure 10</a> to obtain $D_{\mathrm{(play,lang)}}$. See real examples below in <a href="#tab:ex_train_lang">Table 5</a>. Note the language collected has significant diversity in length and content.
</p>

<div class="l-body" id="env">
<figure>
<img id="fig:ui" src="figs/appendix/fig10_collection_tool.png" style="margin: 0; width: 60%;"/>
  <figcaption>
  <b>Figure 10. A schematic rendering of our hindsight language collection tool.</b>
  </figcaption>
</figure>
</div>


<p>
<a href="#fig:ui">Figure 10</a> demonstrates how hindsight instructions are collected. Overseers are presented with a webpage that contains a looping video clip of 1 to 2 seconds of play, along with the first and the last frames of that video, to help them identify the beginning and the end of the clip if necessary.
Overseers are asked to type in a text box the sentence that best answers the question "How do I go from start to finish?".
</p><p>
Several considerations can go into the design of this interface. First, we can ask overseers to type in multiple instructions that are as different from each other as possible, to increase generalization and diversity of the language dataset. After experimenting with one or multiple text boxes, we opted for using only one, while asking users to write as diverse sentences as possible throughout the collection process. A disadvantage of having multiple boxes is that it can sometimes be challenging to come up with diverse sentences when the observed action is very simple. It also leads to less video-language pairs for the same budget. Thus we decided one sentence per video was most beneficial in our case.
</p><p>
    Another collection consideration is the level of details in a sentence. For example, for a generalist robot application, it seems "open the drawer" is a more likely use case than "move your hand 10 cm to the right above the drawer handle, then grasp the drawer handle, then pull on the drawer handle". Similarly, an instruction geared toward a function such as "open the drawer" is more likely useful than one detached from it function, e.g. "put your fingers around the drawer handle, pull your hand back".
</p><p>
    Finally, given the temporal horizon of our video clips, multiple things can be happening within one clip. How many events should be described? For example it might be important to describe multiple motions such as "close the drawer then reach for the object". With all these observations in mind, we instructed the overseers to only describe the main actions, while asking themselves: “which instructions would I give to another human being so that they could produce a similar video if they were in that scene?”.
</p>

<div class="l-body" id="env">
<figure>
<img id="tab:ex_train_lang" src="figs/tables/table5_language_samples.png" style="margin: 0; width: 60%;"/>
  <figcaption>
  <b>Table 5. Randomly sampled examples</b> from the training set of hindsight instructions paired with play. As hindsight instruction pairing sits on top of play, the language we collect is similarly not constrained by predefined tasks, and instead covers both specific functional behaviors and general, non task-specific behaviors.
  </figcaption>
</figure>
</div>

<div class="l-body" id="env">
<figure>
<img id="tab:ex_test_lang" src="figs/tables/table6_18task.png" style="margin: 0; width: 70%;"/>
  <figcaption>
  <b>Table 6.</b> Example natural language instructions used to specify the 18 test-time visual manipulation tasks.
  </figcaption>
</figure>
</div>



<h4>D.3 (Demo, language) dataset</h4>
<p>
To define $D_{\mathrm{(demo,lang)}}$, we pair each of the 100 demonstrations of the 18 evaluation tasks from <d-cite key="lynch2019play"></d-cite> with hindsight instructions using the same process as in <a href="#sec:hip">Section 4.1</a>. We similarly pair the 20 test time demonstrations of each of the 18 tasks. See examples for each of the tasks in <a href="#tab:ex_test_lang">Table 7</a>.
74.3% of the natural language instructions in the test dataset appear at least once in $D_{\mathrm{play}}$.
85.08% of the instructions in $D_{\mathrm{(play,lang)}}$ never never appear in the test set.
</p>
<h4>D.4 Restricted play dataset</h4>
<p>
For a controlled comparison between LangLfP and LangBC, we train on a play dataset restricted to the same size as the aggregate multitask demonstration dataset ($\thicksim$1.5h). This was obtained by randomly subsampling the original play logs $\thicksim$7h to $\thicksim$1.5h before relabeling.
</p>

<h3 id="sec:appendix_models">E. Models</h3>

<p>
Below we describe our various baselines and their training sources. Note that for a controlled comparison, all methods are trained using the exact same architecture described in <a href="#sec:appendix_architecture">Appendix B</a>, differing only on the source of data.
</p>

<div class="l-body" id="env">
<figure>
<img id="tab:ex_test_lang" src="figs/tables/table7_data_sources.png" style="margin: 0; width: 70%;"/>
  <figcaption>
  <b>Table 7. Methods and their training data sources.</b> All baselines are trained to maximize the same generalized contextual imitation objective MCIL.
  </figcaption>
</figure>
</div>

<h3 id="sec:appendix_eval">F. Long Horizon Evaluation</h3>

<p><b>Task construction.</b>
We construct long-horizon evaluations by considering transitions between the 18 subtasks defined in <d-cite key="lynch2019play"></d-cite>. These span multiple diverse families, e.g. opening and closing doors and drawers, pressing buttons, picking and placing a movable block, etc. For example, one of the 925 Chain-4 tasks may be "open_sliding, push_blue, open_drawer, sweep". We exclude as invalid any transitions that would result in instructions that would have necessarily already been satisfied, e.g. "open_drawer, press_red, open_drawer". To allow natural language conditioning we pair 20 test demonstrations of each subtask with human instructions using the same process as in <a href="#sec:hip">Section 4.1</a> (example sentences in <a href="#tab:ex_test_lang">Table 7</a>).
</p>
<p><b>Eval walkthrough.</b>
The long horizon evaluation happens as follows. For each of the N-stage tasks in a given benchmark, we start by resetting the scene to a neutral state. This is discussed more in the next section.
Next, we sample a natural language instruction for each task in the N-stage sequence from a test set. Next, for each subtask in a row, we condition the agent on the current subtask instruction and rollout the policy. If at any timestep the agent successfully completes the current subtask (according to the environment-defined reward), we transition to the next subtask in the sequence (after a half second delay). This attempts to mimic the qualitative scenario where a human provides one instruction after another, queuing up the next instruction, then entering it only once the human has deemed the current subtask solved. If the agent does not complete a given subtask in 8 seconds, the entire episode ends in a timeout.
We score each multi-stage rollout by the percent of subtasks completed, averaging over all multi-stage tasks in the benchmark to arrive at the final N-stage number.
  </p>
  
<p><b>Neutrality in multitask evaluation</b>. When evaluating any context conditioned policy (goal-image, task id, natural language, etc.), a natural question that arises is: how much is the policy relying on the context to infer and solve the task? Previously in <d-cite key="lynch2019play"></d-cite>, evaluation began by resetting the simulator to the first state of a test demonstration, ensuring that the commanded task was valid. We find that under this reset scheme, the initial pose of the agent can in some cases become correlated with the task, e.g. arm nearby the drawer for a drawer opening task. This is problematic, as it potentially reveals task information to the policy through the <i>initial state</i>, rather than the context.
</p>

<p>
In this work, we instead reset the arm to a fixed neutral position at the beginning of every test episode. This "neutral" initialization scheme is used to run all evaluations in this paper. We note this is a fairer, but significantly more difficult benchmark. Neutral initialization breaks correlation between initial state and task, forcing the agent to rely entirely on language to infer and solve the task.
For completeness, we present evaluation curves for both LangLfP and LangBC under this and the prior possibly  "correlated" initialization scheme in <a href="#fig:chains_non_neutral">Figure 11</a>. We see that when LangBC is allowed to start from the exact first state of a demonstration (a rigid real world assumption), it does well on the first task, but fails to transition to the other tasks, with performance degrading quickly after the first instruction (Chain-2, Chain-3, Chain-4). Play-trained LangLfP on the other hand, is far more robust to change in starting position, experiencing only a minor degradation in performance when switching from the correlated initialization to neutral.
</p>

<div class="l-body" id="env">
<figure>
<img id="fig:chains_non_neutral" src="figs/results/fig11_robustness.png" style="margin: 0; width: 80%;"/>
  <figcaption>
  <b>Figure 11. Training on relabeled play leads to robustness.</b> Models trained on relabeled play (LangLfP) are robust to a fixed neutral starting position during multitask evaluation, models trained on conventional predefined demonstrations (LangBC) are not.
  </figcaption>
</figure>
</div>

<h3 id="sec:qualitative_examples">G. Qualitative Examples</h3>

<h4 id="repeated">Repeated Instructions</h4>

Here we demonstrate repetitions of the same pair of actions multiple times: on the left the input sentences ("pick up the object and drop it vertically", "knock the object") is are performed twice in a row, on the right the ("open the drawer", "close the drawer") instructions are repeated 3 times in a row:

<div class="l-page-outset" id="transfer_table">
<table border="0" cellpadding="5" style="text-align: center">
  <tr>
    <td>
      <figure>
      <video class="b-lazy" loop autoplay playsinline muted style="display: block; width: 100%; margin:0 auto" data-src="videos/live/playlang_20200326-114030_knock2_bt300k.mp4" type="video/mp4"></video>
      <figcaption>(upright object, knock object) x 2</figcaption>
      </figure>
    </td>
    <td>
      <figure>
      <video class="b-lazy" loop autoplay playsinline muted style="display: block; width: 100%; margin:0 auto" data-src="videos/live/playlang_20200326-184425_open-close-drawer-3x_bt300k.mp4" type="video/mp4"></video>
      <figcaption>(open drawer, close drawer) x 3</figcaption>
      </figure>
    </td>
  </tr>
  <tr>
    <td>
      <figure>
      <video class="b-lazy" loop autoplay playsinline muted style="display: block; width: 100%; margin:0 auto" data-src="videos/repeat/reapeat_iter52001_chain_demo0_lang-en_trial22_success1.0.mp4_frames_bt300k.mp4" type="video/mp4"></video>
      <figcaption>object (in, out) of shelf x 2</figcaption>
      </figure>
    </td>
    <td>
    </td>
  </tr>
</table>
</div>

<h4 id="more_examples">More examples of compound instructions</h4>

<figure>
<video class="b-lazy" loop autoplay playsinline muted style="display: block; width: 88%; margin:0 auto" data-src="videos/pixels/chain2/chain2_grasp_lift-close_drawer_iter182001_chain_demo0_lang-en_trial0_success1.0.mp4_frames_bt300k.mp4" type="video/mp4"></video>
<figcaption>
</figcaption>
</figure>

<figure>
<video class="b-lazy" loop autoplay playsinline muted style="display: block; width: 88%; margin:0 auto" data-src="videos/pixels/chain2/chain2_put_in_shelf-sliding_iter182001_chain_demo2_lang-en_trial0_success1.0.mp4_frames_bt300k.mp4" type="video/mp4"></video>
<figcaption>
</figcaption>
</figure>

<figure>
<video class="b-lazy" loop autoplay playsinline muted style="display: block; width: 88%; margin:0 auto" data-src="videos/pixels/chain2/chain2_sweep-grasp_lift_iter182001_chain_demo0_lang-en_trial2_success1.0.mp4_frames_bt300k.mp4" type="video/mp4"></video>
<figcaption>
</figcaption>
</figure>

<figure>
  <video class="b-lazy" loop autoplay playsinline muted style="display: block; width: 88%; margin:0 auto" data-src="videos/pixels/chain2/chain2_sweep_right-grasp_flat_iter182001_chain_demo0_lang-en_trial4_success1.0.mp4_frames_bt300k.mp4" type="video/mp4"></video>
<figcaption>
</figcaption>
</figure>


<h3 id="ablation">H. Ablation: How much language is necessary?</h3>
<p>
We study how the performance of LangLfP scales with the number of collected language pairs. <a href="#fig:lang_ablation">Figure 15</a> compares models trained from pixel inputs to models trained from state inputs as we sweep the amount of collected language pairs by factors of 2. Interestingly, for models trained from states, doubling the size of the language dataset from 5k to 10k has marginal benefit on performance. Models trained from pixels, on the other hand, have yet to converge and may benefit from even larger datasets. This suggests that the role of larger language pair datasets is primarily to address the complicated perceptual grounding problem.
</p>

<div class="l-body" id="env">
<figure>
<img id="fig:lang_ablation" src="figs/appendix/fig15_success_vs_amount.png" style="margin: 0; width: 80%;"/>
  <figcaption>
  <b>Figure 15. 18-task success vs amount of human language pairs.</b>
  </figcaption>
</figure>
</div>


<h3 id="transferlanglfp_vs_langlfp">I. Knowledge transfer with language pretraining</h3>
TransferLangLfP, trained on top of multilingual language embeddings, is able to follow instructions in 16 different languages in zero shot. LangLfP, which learns language from scratch, can only follow English instructions it was trained on. We see when presented with cross-lingual inputs, LangLfP resorts to outputting maximum likelihood play actions. TransferLangLfP, on the other hand, correctly solves tasks.

<div class="l-page-outset" id="transfer_table">
<table border="0" cellpadding="0" style="text-align: center">
  <tr>
    <td><big><b>TransferLangLfP</b></big><br>(understands 16 languages)</td>
    <td><big><b>LangLfP</b></big><br>(only understands english)</td>
  </tr>
  <tr>
    <td>
      <figure>
      <video class="b-lazy" loop autoplay playsinline muted style="display: block; width: 100%; margin:0 auto" data-src="videos/pixels/ood_lfpt/ood_lfpt_close_drawer-drawer-sweep-close_drawer_iter182001_chain_demo0_lang-fr_trial1_success1.0.mp4_frames_bt300k.mp4" type="video/mp4"></video>
      </figure>
    </td>
    <td>
      <figure>
      <video class="b-lazy" loop autoplay playsinline muted style="display: block; width: 100%; margin:0 auto" data-src="videos/pixels/ood_lfp/ood_lfp_close_drawer-drawer-sweep-close_drawer_iter330001_chain_demo0_lang-fr_trial1_success0.0.mp4_frames_bt300k.mp4" type="video/mp4"></video>
      </figure>
    </td>
  </tr>
  <tr>
     <td colspan=2><small>
     Input sentences:
      1. close the drawer all the way
      2. pull the drawer handle all the way
      3. drag the object into the drawer
      4. close the drawer
      <br><br></small>
    </td>
  </tr>

  <tr>
    <td>
      <figure>
      <video class="b-lazy" loop autoplay playsinline muted style="display: block; width: 100%; margin:0 auto" data-src="videos/pixels/ood_lfpt/ood_lfpt_close_drawer-sliding-push_green-close_sliding_iter182001_chain_demo0_lang-ja_trial2_success1.0.mp4_frames_bt300k.mp4" type="video/mp4"></video>
      </figure>
    </td>
    <td>
      <figure>
      <video class="b-lazy" loop autoplay playsinline muted style="display: block; width: 100%; margin:0 auto" data-src="videos/pixels/ood_lfp/ood_lfp_close_drawer-sliding-push_green-close_sliding_iter330001_chain_demo0_lang-ja_trial3_success0.2.mp4_frames_bt300k.mp4" type="video/mp4"></video>
      </figure>
    </td>
  </tr>
  <tr>
     <td colspan=2><small>
     Input sentences:
      1. close the drawer all the way
      2. push the door to the right
      3. press green
      4. move the door all the way left
      <br><br></small>
    </td>
  </tr>
  
</table>
</div>



<h3 id="failures">J. More Failure Cases</h3>


<figure>
<video class="b-lazy" loop autoplay playsinline muted style="display: block; width: 88%; margin:0 auto" data-src="videos/pixels/failures/failures_sweep-grasp_lift_iter182001_chain_demo1_lang-en_trial5_success0.5.mp4_frames_bt300k.mp4" type="video/mp4"></video>
<figcaption>
The agent times out attempting to lift the block from the drawer.
</figcaption>
</figure>

<figure>
<video class="b-lazy" loop autoplay playsinline muted style="display: block; width: 88%; margin:0 auto" data-src="videos/pixels/failures/failures_sweep_right-grasp_flat_iter182001_chain_demo1_lang-en_trial1_success0.0.mp4_frames_bt300k.mp4" type="video/mp4"></video>
<figcaption>
The agent encounters an arm configuration likely avoided during teleoperated play, leading to compounding error.
</figcaption>
</figure>

<figure>
<video class="b-lazy" loop autoplay playsinline muted style="display: block; width: 70%; margin:0 auto" data-src="videos/failures/iter52001_chain_demo0_lang-en_trial5_success1.0.mp4_frames_bt300k.mp4" type="video/mp4"></video>
<figcaption>
The agent makes several incorrect choices on the way to solving a task.
</figcaption>
</figure>


</d-article>

<d-appendix>

  <h3 id="acknowledgments">Acknowledgments</h3>
  <p>We thank Karol Hausman, Eric Jang, Mohi Khansari, Kanishka Rao, Jonathan Thompson, Luke Metz, Anelia Angelova, Sergey Levine, and Vincent Vanhoucke for providing helpful feedback on this manuscript. We additionally thank the overseers for providing paired language instructions.</p>

  <p>We thank <a href="https://distill.pub/">distill.pub</a> for providing this <a href="https://github.com/distillpub/template">template</a>.</p>

  <h3 id="citation">Citation</h3>
<p>For attribution in academic contexts, please cite this work as:</p>
<pre class="citation short">Lynch and Sermanet, "Grounding Language in Play", 2020.</pre>
<p>BibTeX citation:</p>
<pre class="citation long">@article{lynch2020language,
  title   = {Grounding Language in Play},
  author  = {Lynch, Corey and Sermanet, Pierre},
  journal = {arXiv preprint arXiv:2005.07648},
  year    = {2020},
  url     = {https://arxiv.org/abs/2005.07648},
  pdf     = {https://arxiv.org/pdf/2005.07648.pdf},
}</pre>



  <d-footnote-list></d-footnote-list>
  <d-citation-list id="references"></d-citation-list>

</d-appendix>

<!-- <d-bibliography src="bibliography.bib"></d-bibliography> -->
<d-bibliography>
<script type="text/bibtex">


@article{McGeer01041990,
  author = {McGeer, Tad}, 
  title = {\href{http://ijr.sagepub.com/content/9/2/62.abstract}{Passive Dynamic Walking}}, 
  volume = {9}, 
  number = {2}, 
  pages = {62-82}, 
  year = {1990}, 
  doi = {10.1177/027836499000900206}, 
  URL = {http://ijr.sagepub.com/content/9/2/62.abstract}, 
  eprint = {http://ijr.sagepub.com/content/9/2/62.full.pdf+html}, 
  journal = {The International Journal of Robotics Research}
}

@article{kalman1960new,
  title={A new approach to linear filtering and prediction problems},
  author={Kalman, R.E.},
  journal={Journal of Basic Engineering},
  volume={82},
  number={1},
  pages={35--45},
  year={1960},
  publisher={Citeseer}
}

@inproceedings{mooney2008learning,
  title={Learning to Connect Language and Perception.},
  author={Mooney, Raymond J},
  booktitle={AAAI},
  pages={1598--1601},
  year={2008}
}

@article{lynch2019play,
  title   = {Learning Latent Plans from Play},
  author  = {Lynch, Corey and Khansari, Mohi and Xiao, Ted and Kumar, Vikash
             and Tompson, Jonathan and Levine, Sergey and Sermanet, Pierre},
  journal = {Conference on Robot Learning (CoRL)},
  year    = {2019},
  url     = {https://arxiv.org/abs/1903.01973},
  pdf     = {https://arxiv.org/pdf/1903.01973.pdf},
  biburl  = {https://github.com/sermanet/sermanet.github.io/blob/master/assets/bib/Lynch2019Play.bib},
}

@article{gupta2019relay,
  title={Relay Policy Learning: Solving Long Horizon Tasks via Imitation and Reinforcement Learning},
  author={Gupta, Abhishek and Kumar, Vikash and Lynch, Corey and Levine, Sergey and Hausman, Karol},
  journal={Conference on Robot Learning (CoRL)},
  year={2019}
}

@inproceedings{TodorovET12,
  added-at = {2018-11-14T00:00:00.000+0100},
  author = {Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  biburl = {https://www.bibsonomy.org/bibtex/22218cb5d1a40b6e6c2b6077fdff08fbf/dblp},
  booktitle = {IROS},
  ee = {https://www.wikidata.org/entity/Q56812558},
  interhash = {869ab290a52bafc47114756b66df3c25},
  intrahash = {2218cb5d1a40b6e6c2b6077fdff08fbf},
  isbn = {978-1-4673-1737-5},
  keywords = {dblp},
  pages = {5026-5033},
  publisher = {IEEE},
  timestamp = {2019-10-17T14:19:43.000+0200},
  title = {MuJoCo: A physics engine for model-based control.},
  url = {http://dblp.uni-trier.de/db/conf/iros/iros2012.html#TodorovET12},
  year = {2012}
}


@inproceedings{ross2011reduction,
  title={A reduction of imitation learning and structured prediction to no-regret online learning},
  author={Ross, St{\'e}phane and Gordon, Geoffrey and Bagnell, Drew},
  booktitle={Proceedings of the fourteenth international conference on artificial intelligence and statistics},
  pages={627--635},
  year={2011}
}

@article{smith2005development,
  title={The development of embodied cognition: Six lessons from babies},
  author={Smith, Linda and Gasser, Michael},
  journal={Artificial life},
  volume={11},
  number={1-2},
  pages={13--29},
  year={2005},
  publisher={MIT Press}
}

@article{harnad1990symbol,
  title={The symbol grounding problem},
  author={Harnad, Stevan},
  journal={Physica D: Nonlinear Phenomena},
  volume={42},
  number={1-3},
  pages={335--346},
  year={1990},
  publisher={Elsevier}
}

@article{clark1991grounding,
  title={Grounding in communication.},
  author={Clark, Herbert H and Brennan, Susan E},
  year={1991},
  publisher={American Psychological Association}
}

@article{baldwin2007inherently,
  title={How inherently social is language?},
  author={Baldwin, Dare and Meyer, Meredith},
  year={2007},
  publisher={Blackwell Publishing}
}

@article{demirezen1988behaviorist,
  title={Behaviorist theory and language learning},
  author={Demirezen, Mehmet},
  journal={Hacettepe {\"U}niversitesi E{\u{g}}itim Fak{\"u}ltesi Dergisi},
  volume={3},
  number={3},
  year={1988}
}

@article{verga2013relevant,
  title={How relevant is social interaction in second language learning?},
  author={Verga, Laura and Kotz, Sonja A},
  journal={Frontiers in human neuroscience},
  volume={7},
  pages={550},
  year={2013},
  publisher={Frontiers}
}

@article{stemmer1973language,
  title={Language acquisition and classical conditioning},
  author={Stemmer, Nathan},
  journal={Language and Speech},
  volume={16},
  number={3},
  pages={279--282},
  year={1973},
  publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}

@inproceedings{das2018embodied,
  title={Embodied question answering},
  author={Das, Abhishek and Datta, Samyak and Gkioxari, Georgia and Lee, Stefan and Parikh, Devi and Batra, Dhruv},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops},
  pages={2054--2063},
  year={2018}
}

@article{hermann2017grounded,
  title={Grounded language learning in a simulated 3d world},
  author={Hermann, Karl Moritz and Hill, Felix and Green, Simon and Wang, Fumin and Faulkner, Ryan and Soyer, Hubert and Szepesvari, David and Czarnecki, Wojciech Marian and Jaderberg, Max and Teplyashin, Denis and others},
  journal={arXiv preprint arXiv:1706.06551},
  year={2017}
}

@misc{2004.10151,
Author = {Yonatan Bisk and Ari Holtzman and Jesse Thomason and Jacob Andreas and Yoshua Bengio and Joyce Chai and Mirella Lapata and Angeliki Lazaridou and Jonathan May and Aleksandr Nisnevich and Nicolas Pinto and Joseph Turian},
Title = {Experience Grounds Language},
Year = {2020},
Eprint = {arXiv:2004.10151},
}

@article{sachs1981language,
  title={Language learning with restricted input: Case studies of two hearing children of deaf parents},
  author={Sachs, Jacqueline and Bard, Barbara and Johnson, Marie L},
  journal={Applied Psycholinguistics},
  volume={2},
  number={1},
  pages={33--54},
  year={1981},
  publisher={Cambridge University Press}
}

@book{o2005children,
  title={How children learn language},
  author={O\'grady, William},
  year={2005},
  publisher={Cambridge University Press}
}

@book{skinner1957verbal,
  title={Verbal behavior},
  author={Skinner, Burrhus Frederic},
  year={1957},
  publisher={New York: Appleton-Century-Crofts}
}

@inproceedings{nair2018visual,
  title={Visual reinforcement learning with imagined goals},
  author={Nair, Ashvin V and Pong, Vitchyr and Dalal, Murtaza and Bahl, Shikhar and Lin, Steven and Levine, Sergey},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9191--9200},
  year={2018}
}

@article{nair2019contextual,
  title={Contextual Imagined Goals for Self-Supervised Robotic Learning},
  author={Nair, Ashvin and Bahl, Shikhar and Khazatsky, Alexander and Pong, Vitchyr and Berseth, Glen and Levine, Sergey},
  journal={arXiv preprint arXiv:1910.11670},
  year={2019}
}

@article{pong2019skew,
  title={Skew-fit: State-covering self-supervised reinforcement learning},
  author={Pong, Vitchyr H and Dalal, Murtaza and Lin, Steven and Nair, Ashvin and Bahl, Shikhar and Levine, Sergey},
  journal={arXiv preprint arXiv:1903.03698},
  year={2019}
}

@article{warde2018unsupervised,
  title={Unsupervised control through non-parametric discriminative rewards},
  author={Warde-Farley, David and Van de Wiele, Tom and Kulkarni, Tejas and Ionescu, Catalin and Hansen, Steven and Mnih, Volodymyr},
  journal={arXiv preprint arXiv:1811.11359},
  year={2018}
}

@article{florensa2019self,
  title={Self-supervised learning of image embedding for continuous control},
  author={Florensa, Carlos and Degrave, Jonas and Heess, Nicolas and Springenberg, Jost Tobias and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1901.00943},
  year={2019}
}

@article{ghosh2019learning,
  title={Learning To Reach Goals Without Reinforcement Learning},
  author={Ghosh, Dibya and Gupta, Abhishek and Fu, Justin and Reddy, Ashwin and Devine, Coline and Eysenbach, Benjamin and Levine, Sergey},
  journal={arXiv preprint arXiv:1912.06088},
  year={2019}
}

@article{eysenbach2020rewriting,
  title={Rewriting History with Inverse RL: Hindsight Inference for Policy Improvement},
  author={Eysenbach, Benjamin and Geng, Xinyang and Levine, Sergey and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:2002.11089},
  year={2020}
}

@inproceedings{kaelbling1993learning,
  title={Learning to achieve goals},
  author={Kaelbling, Leslie Pack},
  booktitle={IJCAI},
  pages={1094--1099},
  year={1993},
  organization={Citeseer}
}

@inproceedings{ding2019goal,
  title={Goal-conditioned imitation learning},
  author={Ding, Yiming and Florensa, Carlos and Abbeel, Pieter and Phielipp, Mariano},
  booktitle={Advances in Neural Information Processing Systems},
  pages={15298--15309},
  year={2019}
}

@inproceedings{ALFRED20,
  title ={{ALFRED: A Benchmark for Interpreting Grounded
           Instructions for Everyday Tasks}},
  author={Mohit Shridhar and Jesse Thomason and
          Daniel Gordon and Yonatan Bisk and
          Winson Han and Roozbeh Mottaghi and
          Luke Zettlemoyer and Dieter Fox},
  booktitle = {The IEEE Conference on Computer Vision
              and Pattern Recognition (CVPR)},
  year = {2020},
  url  = {https://arxiv.org/abs/1912.01734}
}

@article{Sermanet2017TCN,
  author    = {Pierre Sermanet and
               Corey Lynch and
               Yevgen Chebotar and
               Jasmine Hsu and
               Eric Jang and
               Stefan Schaal and
               Sergey Levine},
  title     = {Time-Contrastive Networks: Self-Supervised Learning from Video},
  journal   = {Proceedings of International Conference in Robotics and Automation (ICRA)},
  year      = {2018},
  url       = {http://arxiv.org/abs/1704.06888},
  biburl    = {https://github.com/sermanet/sermanet.github.io/blob/master/assets/bib/Sermanet2017TCN.bib},
}

@article{Sermanet2017Rewards,
  author    = {Pierre Sermanet and
               Kelvin Xu and
               Sergey Levine},
  title     = {Unsupervised Perceptual Rewards for Imitation Learning},
  journal   = {Proceedings of Robotics: Science and Systems (RSS)},
  year      = {2017},
  url       = {http://arxiv.org/abs/1612.06699},
  biburl    = {https://github.com/sermanet/sermanet.github.io/blob/master/assets/bib/Sermanet2017Rewards.bib},
}

@article{kalashnikov2018qt,
  title={Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation},
  author={Kalashnikov, Dmitry and Irpan, Alex and Pastor, Peter and Ibarz, Julian and Herzog, Alexander and Jang, Eric and Quillen, Deirdre and Holly, Ethan and Kalakrishnan, Mrinal and Vanhoucke, Vincent and others},
  journal={arXiv preprint arXiv:1806.10293},
  year={2018}
}

@article{lee2019efficient,
  title={Efficient exploration via state marginal matching},
  author={Lee, Lisa and Eysenbach, Benjamin and Parisotto, Emilio and Xing, Eric and Levine, Sergey and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1906.05274},
  year={2019}
}

@article{macmahon2006walk,
  title={Walk the talk: Connecting language, knowledge, and action in route instructions},
  author={MacMahon, Matt and Stankiewicz, Brian and Kuipers, Benjamin},
  journal={Def},
  volume={2},
  number={6},
  pages={4},
  year={2006}
}

@article{yu2018interactive,
  title={Interactive grounded language acquisition and generalization in a 2d world},
  author={Yu, Haonan and Zhang, Haichao and Xu, Wei},
  journal={arXiv preprint arXiv:1802.01433},
  year={2018}
}

@inproceedings{oh2017zero,
  title={Zero-shot task generalization with multi-task deep reinforcement learning},
  author={Oh, Junhyuk and Singh, Satinder and Lee, Honglak and Kohli, Pushmeet},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={2661--2670},
  year={2017},
  organization={JMLR. org}
}

@article{raffel2019exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={arXiv preprint arXiv:1910.10683},
  year={2019}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI Blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{yang2019multilingual,
  title={Multilingual universal sentence encoder for semantic retrieval},
  author={Yang, Yinfei and Cer, Daniel and Ahmad, Amin and Guo, Mandy and Law, Jax and Constant, Noah and Abrego, Gustavo Hernandez and Yuan, Steve and Tar, Chris and Sung, Yun-Hsuan and others},
  journal={arXiv preprint arXiv:1907.04307},
  year={2019}
}

@inproceedings{rahmatizadeh2018vision,
  title={Vision-based multi-task manipulation for inexpensive robots using end-to-end learning from demonstration},
  author={Rahmatizadeh, Rouhollah and Abolghasemi, Pooya and B{\"o}l{\"o}ni, Ladislau and Levine, Sergey},
  booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={3758--3765},
  year={2018},
  organization={IEEE}
}

@article{caruana1997multitask,
  title={Multitask learning},
  author={Caruana, Rich},
  journal={Machine learning},
  volume={28},
  number={1},
  pages={41--75},
  year={1997},
  publisher={Springer}
}

@inproceedings{schaul2015universal,
  title={Universal value function approximators},
  author={Schaul, Tom and Horgan, Daniel and Gregor, Karol and Silver, David},
  booktitle={International conference on machine learning},
  pages={1312--1320},
  year={2015}
}

@article{yu2019meta,
  title={Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning},
  author={Yu, Tianhe and Quillen, Deirdre and He, Zhanpeng and Julian, Ryan and Hausman, Karol and Finn, Chelsea and Levine, Sergey},
  journal={arXiv preprint arXiv:1910.10897},
  year={2019}
}

@inproceedings{atkeson1997robot,
  title={Robot learning from demonstration},
  author={Atkeson, Christopher G and Schaal, Stefan},
  booktitle={ICML},
  volume={97},
  pages={12--20},
  year={1997},
  organization={Citeseer}
}

@article{argall2009survey,
  title={A survey of robot learning from demonstration},
  author={Argall, Brenna D and Chernova, Sonia and Veloso, Manuela and Browning, Brett},
  journal={Robotics and autonomous systems},
  volume={57},
  number={5},
  pages={469--483},
  year={2009},
  publisher={Elsevier}
}

@inproceedings{zhang2018deep,
  title={Deep imitation learning for complex manipulation tasks from virtual reality teleoperation},
  author={Zhang, Tianhao and McCarthy, Zoe and Jow, Owen and Lee, Dennis and Chen, Xi and Goldberg, Ken and Abbeel, Pieter},
  booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={1--8},
  year={2018},
  organization={IEEE}
}

@article{rajeswaran2017learning,
  title={Learning complex dexterous manipulation with deep reinforcement learning and demonstrations},
  author={Rajeswaran, Aravind and Kumar, Vikash and Gupta, Abhishek and Vezzani, Giulia and Schulman, John and Todorov, Emanuel and Levine, Sergey},
  journal={arXiv preprint arXiv:1709.10087},
  year={2017}
}

@inproceedings{duan2017one,
  title={One-shot imitation learning},
  author={Duan, Yan and Andrychowicz, Marcin and Stadie, Bradly and Ho, OpenAI Jonathan and Schneider, Jonas and Sutskever, Ilya and Abbeel, Pieter and Zaremba, Wojciech},
  booktitle={Advances in neural information processing systems},
  pages={1087--1098},
  year={2017}
}

@article{andrychowicz2020learning,
  title={Learning dexterous in-hand manipulation},
  author={Andrychowicz, OpenAI: Marcin and Baker, Bowen and Chociej, Maciek and Jozefowicz, Rafal and McGrew, Bob and Pachocki, Jakub and Petron, Arthur and Plappert, Matthias and Powell, Glenn and Ray, Alex and others},
  journal={The International Journal of Robotics Research},
  volume={39},
  number={1},
  pages={3--20},
  year={2020},
  publisher={SAGE Publications Sage UK: London, England}
}


@article{haarnoja2018soft,
  title={Soft actor-critic algorithms and applications},
  author={Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and others},
  journal={arXiv preprint arXiv:1812.05905},
  year={2018}
}

@article{levine2016end,
  title={End-to-end training of deep visuomotor policies},
  author={Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
  journal={The Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={1334--1373},
  year={2016},
  publisher={JMLR. org}
}

@article{kober2013reinforcement,
  title={Reinforcement learning in robotics: A survey},
  author={Kober, Jens and Bagnell, J Andrew and Peters, Jan},
  journal={The International Journal of Robotics Research},
  volume={32},
  number={11},
  pages={1238--1274},
  year={2013},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{popov2017data,
  title={Data-efficient deep reinforcement learning for dexterous manipulation},
  author={Popov, Ivaylo and Heess, Nicolas and Lillicrap, Timothy and Hafner, Roland and Barth-Maron, Gabriel and Vecerik, Matej and Lampe, Thomas and Tassa, Yuval and Erez, Tom and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1704.03073},
  year={2017}
}

@article{singh2019end,
  title={End-to-end robotic reinforcement learning without reward engineering},
  author={Singh, Avi and Yang, Larry and Hartikainen, Kristian and Finn, Chelsea and Levine, Sergey},
  journal={arXiv preprint arXiv:1904.07854},
  year={2019}
}

@article{thrun1992efficient,
  title={Efficient exploration in reinforcement learning},
  author={Thrun, Sebastian B},
  year={1992},
  publisher={Citeseer}
}

@article{ebert2018visual,
  title={Visual foresight: Model-based deep reinforcement learning for vision-based robotic control},
  author={Ebert, Frederik and Finn, Chelsea and Dasari, Sudeep and Xie, Annie and Lee, Alex and Levine, Sergey},
  journal={arXiv preprint arXiv:1812.00568},
  year={2018}
}

@inproceedings{hester2018deep,
  title={Deep q-learning from demonstrations},
  author={Hester, Todd and Vecerik, Matej and Pietquin, Olivier and Lanctot, Marc and Schaul, Tom and Piot, Bilal and Horgan, Dan and Quan, John and Sendonaris, Andrew and Osband, Ian and others},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

@inproceedings{teh2017distral,
  title={Distral: Robust multitask reinforcement learning},
  author={Teh, Yee and Bapst, Victor and Czarnecki, Wojciech M and Quan, John and Kirkpatrick, James and Hadsell, Raia and Heess, Nicolas and Pascanu, Razvan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4496--4506},
  year={2017}
}

@article{ha2018world,
  title={World models},
  author={Ha, David and Schmidhuber, J{\"u}rgen},
  journal={arXiv preprint arXiv:1803.10122},
  year={2018}
}

@inproceedings{oh2015action,
  title={Action-conditional video prediction using deep networks in atari games},
  author={Oh, Junhyuk and Guo, Xiaoxiao and Lee, Honglak and Lewis, Richard L and Singh, Satinder},
  booktitle={Advances in neural information processing systems},
  pages={2863--2871},
  year={2015}
}

@article{pong2018temporal,
  title={Temporal difference models: Model-free deep rl for model-based control},
  author={Pong, Vitchyr and Gu, Shixiang and Dalal, Murtaza and Levine, Sergey},
  journal={arXiv preprint arXiv:1802.09081},
  year={2018}
}

@inproceedings{andrychowicz2017hindsight,
  title={Hindsight experience replay},
  author={Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, OpenAI Pieter and Zaremba, Wojciech},
  booktitle={Advances in neural information processing systems},
  pages={5048--5058},
  year={2017}
}

@inproceedings{kolter2009near,
  title={Near-Bayesian exploration in polynomial time},
  author={Kolter, J Zico and Ng, Andrew Y},
  booktitle={Proceedings of the 26th annual international conference on machine learning},
  pages={513--520},
  year={2009}
}

@inproceedings{oudeyer2008can,
  title={How can we define intrinsic motivation},
  author={Oudeyer, Pierre-Yves and Kaplan, Frederic and others},
  booktitle={Proc. of the 8th Conf. on Epigenetic Robotics},
  volume={5},
  pages={29--31},
  year={2008}
}

@article{journals/tamd/Schmidhuber10,
  added-at = {2018-11-14T00:00:00.000+0100},
  author = {Schmidhuber, Jürgen},
  biburl = {https://www.bibsonomy.org/bibtex/24c1e5293ff0e808b4167206ab2c87004/dblp},
  ee = {https://www.wikidata.org/entity/Q55924369},
  interhash = {cf1f71c67e2f9b734a048d0bfa69174d},
  intrahash = {4c1e5293ff0e808b4167206ab2c87004},
  journal = {IEEE Trans. Autonomous Mental Development},
  keywords = {dblp},
        number = {3},
  pages = {230-247},
  timestamp = {2018-11-15T12:27:38.000+0100},
  title = {Formal Theory of Creativity, Fun, and Intrinsic Motivation (1990-2010).},
  url = {http://dblp.uni-trier.de/db/journals/tamd/tamd2.html#Schmidhuber10},
         volume = {2},
         year = {2010}
}

@inproceedings{ozair2019wasserstein,
  title={Wasserstein dependency measure for representation learning},
  author={Ozair, Sherjil and Lynch, Corey and Bengio, Yoshua and Van den Oord, Aaron and Levine, Sergey and Sermanet, Pierre},
  booktitle={Advances in Neural Information Processing Systems},
  pages={15578--15588},
  year={2019}
}

@article{oord2018representation,
  title={Representation learning with contrastive predictive coding},
  author={Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1807.03748},
  year={2018}
}

@article{handa2019dexpilot,
  title={Dexpilot: Vision based teleoperation of dexterous robotic hand-arm system},
  author={Handa, Ankur and Van Wyk, Karl and Yang, Wei and Liang, Jacky and Chao, Yu-Wei and Wan, Qian and Birchfield, Stan and Ratliff, Nathan and Fox, Dieter},
  journal={arXiv preprint arXiv:1910.03135},
  year={2019}
}

@article{singh2020scalable,
  title={Scalable Multi-Task Imitation Learning with Autonomous Improvement},
  author={Singh, Avi and Jang, Eric and Irpan, Alexander and Kappler, Daniel and Dalal, Murtaza and Levine, Sergey and Khansari, Mohi and Finn, Chelsea},
  journal={arXiv preprint arXiv:2003.02636},
  year={2020}
}

@article{pirk2019online,
  title={Online Object Representations with Contrastive Learning},
  author={Pirk, S{\"o}ren and Khansari, Mohi and Bai, Yunfei and Lynch, Corey and Sermanet, Pierre},
  journal={arXiv preprint arXiv:1906.04312},
  year={2019}
}

@article{winograd1972understanding,
  title={Understanding natural language},
  author={Winograd, Terry},
  journal={Cognitive psychology},
  volume={3},
  number={1},
  pages={1--191},
  year={1972},
  publisher={Elsevier}
}

@article{luketina2019survey,
  title={A survey of reinforcement learning informed by natural language},
  author={Luketina, Jelena and Nardelli, Nantas and Farquhar, Gregory and Foerster, Jakob and Andreas, Jacob and Grefenstette, Edward and Whiteson, Shimon and Rockt{\"a}schel, Tim},
  journal={arXiv preprint arXiv:1906.03926},
  year={2019}
}

@article{hafner2018learning,
  title={Learning latent dynamics for planning from pixels},
  author={Hafner, Danijar and Lillicrap, Timothy and Fischer, Ian and Villegas, Ruben and Ha, David and Lee, Honglak and Davidson, James},
  journal={arXiv preprint arXiv:1811.04551},
  year={2018}
}

@inproceedings{pathak2017curiosity,
  title={Curiosity-driven exploration by self-supervised prediction},
  author={Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A and Darrell, Trevor},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops},
  pages={16--17},
  year={2017}
}

@inproceedings{kollar2010toward,
  title={Toward understanding natural language directions},
  author={Kollar, Thomas and Tellex, Stefanie and Roy, Deb and Roy, Nicholas},
  booktitle={2010 5th ACM/IEEE International Conference on Human-Robot Interaction (HRI)},
  pages={259--266},
  year={2010},
  organization={IEEE}
}

@article{misra2017mapping,
  title={Mapping instructions and visual observations to actions with reinforcement learning},
  author={Misra, Dipendra and Langford, John and Artzi, Yoav},
  journal={arXiv preprint arXiv:1704.08795},
  year={2017}
}

@article{yu2017deep,
  title={A deep compositional framework for human-like language acquisition in virtual environment},
  author={Yu, Haonan and Zhang, Haichao and Xu, Wei},
  journal={arXiv preprint arXiv:1703.09831},
  year={2017}
}

@inproceedings{wang2019reinforced,
  title={Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation},
  author={Wang, Xin and Huang, Qiuyuan and Celikyilmaz, Asli and Gao, Jianfeng and Shen, Dinghan and Wang, Yuan-Fang and Wang, William Yang and Zhang, Lei},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6629--6638},
  year={2019}
}

@article{hill2019emergent,
  title={Emergent systematic generalization in a situated agent},
  author={Hill, Felix and Lampinen, Andrew and Schneider, Rosalia and Clark, Stephen and Botvinick, Matthew and McClelland, James L and Santoro, Adam},
  journal={arXiv preprint arXiv:1910.00571},
  year={2019}
}

@inproceedings{chaplot2018gated,
  title={Gated-attention architectures for task-oriented language grounding},
  author={Chaplot, Devendra Singh and Sathyendra, Kanthashree Mysore and Pasumarthi, Rama Kumar and Rajagopal, Dheeraj and Salakhutdinov, Ruslan},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

@misc{1903.02020,
Author = {Prasoon Goyal and Scott Niekum and Raymond J. Mooney},
Title = {Using Natural Language for Reward Shaping in Reinforcement Learning},
Year = {2019},
Eprint = {arXiv:1903.02020},
}

@misc{1611.01796,
Author = {Jacob Andreas and Dan Klein and Sergey Levine},
Title = {Modular Multitask Reinforcement Learning with Policy Sketches},
Year = {2016},
Eprint = {arXiv:1611.01796},
}

@misc{1906.07343,
Author = {Yiding Jiang and Shixiang Gu and Kevin Murphy and Chelsea Finn},
Title = {Language as an Abstraction for Hierarchical Deep Reinforcement Learning},
Year = {2019},
Eprint = {arXiv:1906.07343},
}

@misc{1901.05287,
Author = {Yoav Goldberg},
Title = {Assessing BERT\'s Syntactic Abilities},
Year = {2019},
Eprint = {arXiv:1901.05287},
}

@misc{1801.06146,
Author = {Jeremy Howard and Sebastian Ruder},
Title = {Universal Language Model Fine-tuning for Text Classification},
Year = {2018},
Eprint = {arXiv:1801.06146},
}

@misc{1905.06316,
Author = {Ian Tenney and Patrick Xia and Berlin Chen and Alex Wang and Adam Poliak and R Thomas McCoy and Najoung Kim and Benjamin Van Durme and Samuel R. Bowman and Dipanjan Das and Ellie Pavlick},
Title = {What do you learn from context? Probing for sentence structure in contextualized word representations},
Year = {2019},
Eprint = {arXiv:1905.06316},
}

@article{zellers2018swag,
  title={Swag: A large-scale adversarial dataset for grounded commonsense inference},
  author={Zellers, Rowan and Bisk, Yonatan and Schwartz, Roy and Choi, Yejin},
  journal={arXiv preprint arXiv:1808.05326},
  year={2018}
}

@inproceedings{tan2018survey,
  title={A survey on deep transfer learning},
  author={Tan, Chuanqi and Sun, Fuchun and Kong, Tao and Zhang, Wenchang and Yang, Chao and Liu, Chunfang},
  booktitle={International conference on artificial neural networks},
  pages={270--279},
  year={2018},
  organization={Springer}
}

@article{1811.06711,
Author = {Takayuki Osa and Joni Pajarinen and Gerhard Neumann and J. Andrew Bagnell and Pieter Abbeel and Jan Peters},
Title = {An Algorithmic Perspective on Imitation Learning},
Year = {2018},
Eprint = {arXiv:1811.06711},
Doi = {10.1561/2300000053},
}

@misc{1701.05517,
Author = {Tim Salimans and Andrej Karpathy and Xi Chen and Diederik P. Kingma},
Title = {PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications},
Year = {2017},
Eprint = {arXiv:1701.05517},
}

@inproceedings{gu2017deep,
  title={Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates},
  author={Gu, Shixiang and Holly, Ethan and Lillicrap, Timothy and Levine, Sergey},
  booktitle={2017 IEEE international conference on robotics and automation (ICRA)},
  pages={3389--3396},
  year={2017},
  organization={IEEE}
}

@article{parisotto2015actor,
  title={Actor-mimic: Deep multitask and transfer reinforcement learning},
  author={Parisotto, Emilio and Ba, Jimmy Lei and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1511.06342},
  year={2015}
}

@article{schaul2019ray,
  title={Ray interference: a source of plateaus in deep reinforcement learning},
  author={Schaul, Tom and Borsa, Diana and Modayil, Joseph and Pascanu, Razvan},
  journal={arXiv preprint arXiv:1904.11455},
  year={2019}
}

@article{sennrich2015neural,
  title={Neural machine translation of rare words with subword units},
  author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  journal={arXiv preprint arXiv:1508.07909},
  year={2015}
}


</script>


</d-bibliography>


</body>
